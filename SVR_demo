from random import uniform
import pandas as pd
import numpy as np
from sklearn.svm import SVR
from sklearn.model_selection import (RandomizedSearchCV, train_test_split, 
                                   GridSearchCV, cross_val_score, 
                                   cross_validate, KFold)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Tuple, Optional, Dict
import warnings
import time
warnings.filterwarnings('ignore')

class EnhancedSVR:
    """
    Enhanced SVR tool with K-fold cross-validation for overfitting analysis and clean reporting.
    """
    
    def __init__(self):
        self.data = None
        self.X = None
        self.y = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.model = None
        self.feature_names = None
        self.target_name = None
        self.best_params = None
        self.cv_results = None
        self.model_report = {}
        
    def load_data(self, file_path: str) -> pd.DataFrame:
        """Load data from CSV file."""
        try:
            self.data = pd.read_csv(file_path)
            print(f"‚úÖ Data loaded successfully! Shape: {self.data.shape}")
            print(f"üìä Columns: {list(self.data.columns)}")
            return self.data
        except Exception as e:
            print(f"‚ùå Error loading data: {e}")
            return None
    
    def explore_data(self) -> None:
        """Basic data exploration with clean output."""
        if self.data is None:
            print("‚ùå No data loaded. Please load data first.")
            return
        
        print("\n" + "="*60)
        print("üìä DATA EXPLORATION")
        print("="*60)
        
        print(f"Dataset shape: {self.data.shape}")
        print(f"Missing values: {self.data.isnull().sum().sum()}")
        
        if self.data.isnull().sum().sum() > 0:
            print("\n‚ö†Ô∏è Missing values by column:")
            missing = self.data.isnull().sum()
            for col, count in missing[missing > 0].items():
                print(f"   {col}: {count} ({count/len(self.data)*100:.1f}%)")
        
        # Basic statistics for numeric columns
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        print(f"\nüìà Numeric columns: {len(numeric_cols)}")
        print(self.data[numeric_cols].describe().round(2))
        
        # Correlation heatmap
        if len(numeric_cols) > 1:
            plt.figure(figsize=(10, 8))
            correlation_matrix = self.data[numeric_cols].corr()
            mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
            sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', 
                       center=0, fmt='.2f', square=True)
            plt.title('Correlation Matrix')
            plt.tight_layout()
            plt.show()
    
    def prepare_data(self, feature_columns: List[str], target_column: str, 
                    test_size: float = 0.2, random_state: int = 42) -> None:
        """Prepare features and target for training."""
        if self.data is None:
            print("‚ùå No data loaded. Please load data first.")
            return
        
        # Validate columns
        missing_cols = [col for col in feature_columns + [target_column] 
                       if col not in self.data.columns]
        if missing_cols:
            print(f"‚ùå Missing columns: {missing_cols}")
            return
        
        # Select features and target
        self.X = self.data[feature_columns].copy()
        self.y = self.data[target_column].copy()
        self.feature_names = feature_columns
        self.target_name = target_column
        
        # Handle missing values
        self.X = self.X.fillna(self.X.mean())
        self.y = self.y.fillna(self.y.mean())
        
        # Split data
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y, test_size=test_size, random_state=random_state
        )
        
        # Scale features and target
        self.X_train_scaled = self.scaler_X.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler_X.transform(self.X_test)
        self.y_train_scaled = self.scaler_y.fit_transform(self.y_train.values.reshape(-1, 1)).ravel()
        self.y_test_scaled = self.scaler_y.transform(self.y_test.values.reshape(-1, 1)).ravel()
        
        print(f"‚úÖ Data prepared successfully!")
        print(f"   Features: {len(feature_columns)} ({', '.join(feature_columns[:3])}{'...' if len(feature_columns) > 3 else ''})")
        print(f"   Target: {target_column}")
        print(f"   Training: {len(self.X_train)} samples")
        print(f"   Testing: {len(self.X_test)} samples")
    
    def train_model(self, kernel: str = 'rbf', C: float = 1.0, 
                   gamma: str = 'scale', epsilon: float = 0.1) -> None:
        """Train SVR model with specified parameters."""
        if self.X_train is None:
            print("‚ùå No training data prepared. Please prepare data first.")
            return
        
        self.model = SVR(kernel=kernel, C=C, gamma=gamma, epsilon=epsilon)
        self.model.fit(self.X_train_scaled, self.y_train_scaled)
        
        print(f"‚úÖ SVR model trained successfully!")
        print(f"   Kernel: {kernel}, C: {C}, Gamma: {gamma}, Epsilon: {epsilon}")
        print(f"   Support vectors: {self.model.support_vectors_.shape[0]}")
    
    def tune_hyperparameters(self, param_grid: Optional[Dict] = None, 
                           cv_folds: int = 5, quick_mode: bool = True) -> None:
        """Hyperparameter tuning with progress tracking."""
        if self.X_train is None:
            print("‚ùå No training data prepared. Please prepare data first.")
            return
        
        if param_grid is None:
            if quick_mode:
                param_grid = {
                    'kernel': ['rbf', 'linear'],
                    'C': [0.1, 1, 10, 100],
                    'gamma': ['scale', 'auto'],
                    'epsilon': [0.01, 0.1, 0.2]
                }
            else:
                param_grid = {
                    'kernel': ['rbf', 'linear', 'poly'],
                    'C': [0.01, 0.1, 1, 10, 100],
                    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
                    'epsilon': [0.001, 0.01, 0.1, 0.2, 0.5]
                }
        
        total_combinations = np.prod([len(values) for values in param_grid.values()])
        print(f"üîÑ Starting hyperparameter tuning...")
        print(f"   Parameter combinations: {total_combinations}")
        print(f"   CV folds: {cv_folds}")
        print(f"   Mode: {'Quick' if quick_mode else 'Comprehensive'}")
        
        grid_search = GridSearchCV(
            SVR(max_iter=1000), param_grid, 
            cv=cv_folds, scoring='r2', n_jobs=-1, verbose=1
        )
        
        start_time = time.time()
        grid_search.fit(self.X_train_scaled, self.y_train_scaled)
        elapsed_time = time.time() - start_time
        
        self.model = grid_search.best_estimator_
        self.best_params = grid_search.best_params_
        
        print(f"‚úÖ Hyperparameter tuning completed!")
        print(f"   Time: {elapsed_time/60:.1f} minutes")
        print(f"   Best CV score: {grid_search.best_score_:.4f}")
        print(f"   Best parameters: {self.best_params}")
    
    def cross_validate_model(self, cv_folds: int = 5, 
                           model_params: Optional[Dict] = None) -> Dict:
        """Perform K-fold cross-validation for overfitting analysis."""
        if self.X is None:
            print("‚ùå No data prepared. Please prepare data first.")
            return None
        
        # Use current model params if not specified
        if model_params is None:
            if self.model is None:
                model_params = {'kernel': 'rbf', 'C': 1, 'gamma': 'scale', 'epsilon': 0.1}
            else:
                model_params = self.model.get_params()
        
        print(f"üîÑ Performing {cv_folds}-fold cross-validation...")
        
        # Prepare full dataset
        X_scaled = self.scaler_X.fit_transform(self.X)
        y_scaled = self.scaler_y.fit_transform(self.y.values.reshape(-1, 1)).ravel()
        
        # Create model
        model = SVR(**model_params)
        
        # Perform cross-validation
        scoring = ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error']
        cv_results = cross_validate(
            model, X_scaled, y_scaled, 
            cv=KFold(n_splits=cv_folds, shuffle=True, random_state=42),
            scoring=scoring, return_train_score=True, n_jobs=-1
        )
        
        # Process results
        results = {}
        for metric in scoring:
            clean_name = metric.replace('neg_', '').replace('_', ' ').title()
            test_scores = -cv_results[f'test_{metric}'] if 'neg_' in metric else cv_results[f'test_{metric}']
            train_scores = -cv_results[f'train_{metric}'] if 'neg_' in metric else cv_results[f'train_{metric}']
            
            results[f'{metric}_test_mean'] = np.mean(test_scores)
            results[f'{metric}_test_std'] = np.std(test_scores)
            results[f'{metric}_train_mean'] = np.mean(train_scores)
            results[f'{metric}_train_std'] = np.std(train_scores)
            results[f'{metric}_overfitting'] = self._calculate_overfitting_score(
                np.mean(train_scores), np.mean(test_scores), metric
            )
        
        self.cv_results = results
        print("‚úÖ Cross-validation completed!")
        
        return results
    
    def _calculate_overfitting_score(self, train_mean: float, test_mean: float, metric: str) -> float:
        """Calculate overfitting score based on train-test gap."""
        if 'r2' in metric:
            return ((train_mean - test_mean) / abs(train_mean)) * 100 if train_mean != 0 else 0
        else:  # For error metrics (MSE, MAE)
            return ((test_mean - train_mean) / train_mean) * 100 if train_mean != 0 else 0
    
    def evaluate_test_set(self) -> Dict:
        """Evaluate model on test set."""
        if self.model is None:
            print("‚ùå No model trained. Please train a model first.")
            return None
        
        # Predictions
        y_train_pred_scaled = self.model.predict(self.X_train_scaled)
        y_test_pred_scaled = self.model.predict(self.X_test_scaled)
        
        # Transform back to original scale
        y_train_pred = self.scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).ravel()
        y_test_pred = self.scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).ravel()
        
        # Calculate metrics
        results = {
            'train_r2': r2_score(self.y_train, y_train_pred),
            'test_r2': r2_score(self.y_test, y_test_pred),
            'train_rmse': np.sqrt(mean_squared_error(self.y_train, y_train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(self.y_test, y_test_pred)),
            'train_mae': mean_absolute_error(self.y_train, y_train_pred),
            'test_mae': mean_absolute_error(self.y_test, y_test_pred)
        }
        
        # Store predictions for plotting
        self.y_train_pred = y_train_pred
        self.y_test_pred = y_test_pred
        
        return results
    
    def generate_report(self, include_cv: bool = True, cv_folds: int = 5, 
                       include_feature_importance: bool = True) -> Dict:
        """Generate comprehensive model performance report."""
        if self.model is None:
            print("‚ùå No model trained. Please train a model first.")
            return None
        
        print("\n" + "="*70)
        print("üìã SVR MODEL PERFORMANCE REPORT")
        print("="*70)
        
        report = {}
        
        # 1. Model Information
        print("\nüîß MODEL CONFIGURATION:")
        params = self.model.get_params()
        for key, value in params.items():
            if key in ['kernel', 'C', 'gamma', 'epsilon']:
                print(f"   {key.capitalize()}: {value}")
        print(f"   Support vectors: {len(self.model.support_vectors_)}")
        report['model_params'] = params
        
        # 2. Test Set Performance
        print("\nüìä TEST SET PERFORMANCE:")
        test_results = self.evaluate_test_set()
        print(f"   R¬≤ Score: {test_results['test_r2']:.4f}")
        print(f"   RMSE: {test_results['test_rmse']:.4f}")
        print(f"   MAE: {test_results['test_mae']:.4f}")
        report['test_performance'] = test_results
        
        # 3. Cross-Validation Analysis
        if include_cv:
            print("\nüîÑ CROSS-VALIDATION ANALYSIS:")
            cv_results = self.cross_validate_model(cv_folds=cv_folds)
            
            print(f"   R¬≤ (CV): {cv_results['r2_test_mean']:.4f} ¬± {cv_results['r2_test_std']:.4f}")
            print(f"   RMSE (CV): {cv_results['neg_mean_squared_error_test_mean']**.5:.4f} ¬± {cv_results['neg_mean_squared_error_test_std']**.5:.4f}")
            
            # Overfitting analysis
            r2_gap = cv_results['r2_overfitting']
            print(f"\n‚öñÔ∏è  OVERFITTING ANALYSIS ({cv_folds}-fold CV):")
            print(f"   R¬≤ Gap: {r2_gap:+.2f}%")
            
            if abs(r2_gap) < 5:
                status = "‚úÖ Excellent generalization"
            elif abs(r2_gap) < 15:
                status = "‚ö†Ô∏è  Moderate overfitting"
            else:
                status = "‚ùå High overfitting detected"
            print(f"   Status: {status}")
            
            report['cross_validation'] = cv_results
            report['overfitting_status'] = status
        
        # 4. Feature Importance
        if include_feature_importance:
            print("\nüîç FEATURE IMPORTANCE:")
            try:
                perm_importance = permutation_importance(
                    self.model, self.X_test_scaled, self.y_test_scaled,
                    n_repeats=5, random_state=42, n_jobs=-1
                )
                
                importance_df = pd.DataFrame({
                    'feature': self.feature_names,
                    'importance': perm_importance.importances_mean
                }).sort_values('importance', ascending=False)
                
                for i, row in importance_df.head(5).iterrows():
                    print(f"   {row['feature']}: {row['importance']:.4f}")
                
                report['feature_importance'] = importance_df
            except Exception as e:
                print(f"   ‚ö†Ô∏è Could not calculate feature importance: {e}")
        
        # 5. Recommendations
        print("\nüí° RECOMMENDATIONS:")
        recommendations = self._generate_recommendations(report)
        for rec in recommendations:
            print(f"   ‚Ä¢ {rec}")
        report['recommendations'] = recommendations
        
        # 6. Overall Score
        overall_score = self._calculate_overall_score(report)
        print(f"\n‚≠ê OVERALL MODEL SCORE: {overall_score}/10")
        report['overall_score'] = overall_score
        
        self.model_report = report
        return report
    
    def _generate_recommendations(self, report: Dict) -> List[str]:
        """Generate actionable recommendations."""
        recommendations = []
        
        # Performance recommendations
        if 'test_performance' in report:
            test_r2 = report['test_performance']['test_r2']
            if test_r2 < 0.6:
                recommendations.append("Consider feature engineering or trying different kernels")
            elif test_r2 > 0.9:
                recommendations.append("Excellent performance - model is ready for deployment")
        
        # Overfitting recommendations
        if 'cross_validation' in report:
            r2_gap = report['cross_validation']['r2_overfitting']
            if abs(r2_gap) > 15:
                recommendations.append("Reduce overfitting: decrease C or increase epsilon")
            elif abs(r2_gap) > 5:
                recommendations.append("Monitor overfitting: consider regularization")
        
        # Feature recommendations
        if 'feature_importance' in report:
            low_importance = report['feature_importance']['importance'] < 0.01
            if low_importance.sum() > 0:
                recommendations.append(f"Consider removing {low_importance.sum()} low-importance features")
        
        if not recommendations:
            recommendations.append("Model shows good performance and generalization")
        
        return recommendations
    
    def drop_features(self, features_to_drop: List[str], retrain: bool = True) -> None:
        """Manually drop specified features and optionally retrain model."""
        if self.feature_names is None:
            print("‚ùå No features prepared. Please prepare data first.")
            return
        
        # Validate features to drop
        invalid_features = [f for f in features_to_drop if f not in self.feature_names]
        if invalid_features:
            print(f"‚ö†Ô∏è Invalid features (not found): {invalid_features}")
            return
        
        # Remove features
        remaining_features = [f for f in self.feature_names if f not in features_to_drop]
        if len(remaining_features) == 0:
            print("‚ùå Cannot drop all features!")
            return
        
        print(f"üóëÔ∏è Dropping {len(features_to_drop)} features: {features_to_drop}")
        print(f"‚úÖ Remaining {len(remaining_features)} features: {remaining_features}")
        
        # Update data
        self.prepare_data(remaining_features, self.target_name)
        
        # Retrain model if requested
        if retrain and self.model is not None:
            old_params = self.model.get_params()
            print("üîÑ Retraining model with same parameters...")
            self.train_model(**{k: v for k, v in old_params.items() 
                              if k in ['kernel', 'C', 'gamma', 'epsilon']})
    
    def drop_low_importance_features(self, threshold: float = 0.01, 
                                   max_features_to_drop: int = None, 
                                   retrain: bool = True) -> List[str]:
        """Drop features with importance below threshold."""
        if self.model is None:
            print("‚ùå No model trained. Please train a model first.")
            return []
        
        print(f"üîç Analyzing feature importance (threshold: {threshold})...")
        
        # Calculate feature importance
        try:
            perm_importance = permutation_importance(
                self.model, self.X_test_scaled, self.y_test_scaled,
                n_repeats=5, random_state=42, n_jobs=-1
            )
            
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': perm_importance.importances_mean
            }).sort_values('importance', ascending=True)
            
            # Find low importance features
            low_importance_features = importance_df[
                importance_df['importance'] < threshold
            ]['feature'].tolist()
            
            if max_features_to_drop is not None:
                low_importance_features = low_importance_features[:max_features_to_drop]
            
            if not low_importance_features:
                print(f"‚úÖ No features found below threshold {threshold}")
                return []
            
            print(f"üìä Low importance features found:")
            for feature in low_importance_features:
                imp_value = importance_df[importance_df['feature'] == feature]['importance'].iloc[0]
                print(f"   ‚Ä¢ {feature}: {imp_value:.4f}")
            
            # Drop features
            if len(low_importance_features) < len(self.feature_names):
                self.drop_features(low_importance_features, retrain=retrain)
            else:
                print("‚ö†Ô∏è Cannot drop all features - keeping at least one feature")
                low_importance_features = low_importance_features[:-1]
                if low_importance_features:
                    self.drop_features(low_importance_features, retrain=retrain)
            
            return low_importance_features
            
        except Exception as e:
            print(f"‚ùå Error calculating feature importance: {e}")
            return []
    
    def drop_highly_correlated_features(self, correlation_threshold: float = 0.95,
                                      retrain: bool = True) -> List[str]:
        """Drop features that are highly correlated with others."""
        if self.X is None:
            print("‚ùå No data prepared. Please prepare data first.")
            return []
        
        print(f"üîç Analyzing feature correlations (threshold: {correlation_threshold})...")
        
        # Calculate correlation matrix
        correlation_matrix = self.X.corr().abs()
        
        # Find highly correlated features
        features_to_drop = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                if correlation_matrix.iloc[i, j] >= correlation_threshold:
                    feature_i = correlation_matrix.columns[i]
                    feature_j = correlation_matrix.columns[j]
                    
                    # Keep the feature with higher importance (if available)
                    if hasattr(self, 'model_report') and 'feature_importance' in self.model_report:
                        importance_df = self.model_report['feature_importance']
                        imp_i = importance_df[importance_df['feature'] == feature_i]['importance'].iloc[0]
                        imp_j = importance_df[importance_df['feature'] == feature_j]['importance'].iloc[0]
                        
                        drop_feature = feature_i if imp_i < imp_j else feature_j
                    else:
                        # If no importance data, drop the second feature
                        drop_feature = feature_j
                    
                    if drop_feature not in features_to_drop:
                        features_to_drop.append(drop_feature)
                        print(f"   üìä High correlation ({correlation_matrix.iloc[i, j]:.3f}): "
                              f"{feature_i} ‚Üî {feature_j} ‚Üí dropping {drop_feature}")
        
        if not features_to_drop:
            print(f"‚úÖ No highly correlated features found (threshold: {correlation_threshold})")
            return []
        
        print(f"üóëÔ∏è Found {len(features_to_drop)} highly correlated features to drop")
        
        # Drop features
        self.drop_features(features_to_drop, retrain=retrain)
        return features_to_drop
    
    def feature_selection_analysis(self, correlation_threshold: float = 0.95,
                                 importance_threshold: float = 0.01,
                                 show_recommendations: bool = True) -> Dict:
        """Comprehensive feature selection analysis with recommendations."""
        if self.model is None:
            print("‚ùå No model trained. Please train a model first.")
            return {}
        
        print("\n" + "="*60)
        print("üîç FEATURE SELECTION ANALYSIS")
        print("="*60)
        
        analysis_results = {}
        
        # 1. Feature Importance Analysis
        print("\n1Ô∏è‚É£ FEATURE IMPORTANCE ANALYSIS:")
        try:
            perm_importance = permutation_importance(
                self.model, self.X_test_scaled, self.y_test_scaled,
                n_repeats=5, random_state=42, n_jobs=-1
            )
            
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': perm_importance.importances_mean,
                'importance_std': perm_importance.importances_std
            }).sort_values('importance', ascending=False)
            
            print(f"üìä Feature importance scores:")
            for _, row in importance_df.iterrows():
                status = "üî¥" if row['importance'] < importance_threshold else "üü¢"
                print(f"   {status} {row['feature']}: {row['importance']:.4f} ¬± {row['importance_std']:.4f}")
            
            low_importance = importance_df[importance_df['importance'] < importance_threshold]
            analysis_results['low_importance_features'] = low_importance['feature'].tolist()
            analysis_results['importance_df'] = importance_df
            
        except Exception as e:
            print(f"‚ùå Error in importance analysis: {e}")
            analysis_results['low_importance_features'] = []
        
        # 2. Correlation Analysis  
        print(f"\n2Ô∏è‚É£ CORRELATION ANALYSIS (threshold: {correlation_threshold}):")
        correlation_matrix = self.X.corr().abs()
        high_corr_pairs = []
        
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if corr_value >= correlation_threshold:
                    feature_i = correlation_matrix.columns[i]
                    feature_j = correlation_matrix.columns[j]
                    high_corr_pairs.append((feature_i, feature_j, corr_value))
                    print(f"   üî¥ High correlation: {feature_i} ‚Üî {feature_j} ({corr_value:.3f})")
        
        if not high_corr_pairs:
            print(f"   ‚úÖ No highly correlated features found")
        
        analysis_results['high_correlation_pairs'] = high_corr_pairs
        
        # 3. Statistical Summary
        print(f"\n3Ô∏è‚É£ STATISTICAL SUMMARY:")
        print(f"   Total features: {len(self.feature_names)}")
        print(f"   Low importance features: {len(analysis_results['low_importance_features'])}")
        print(f"   High correlation pairs: {len(high_corr_pairs)}")
        
        # 4. Recommendations
        if show_recommendations:
            print(f"\nüí° FEATURE SELECTION RECOMMENDATIONS:")
            recommendations = []
            
            if analysis_results['low_importance_features']:
                recommendations.append(f"Consider dropping {len(analysis_results['low_importance_features'])} low-importance features")
                
            if high_corr_pairs:
                recommendations.append(f"Review {len(high_corr_pairs)} highly correlated feature pairs")
                
            if len(self.feature_names) > 10:
                recommendations.append("Consider dimensionality reduction for large feature sets")
                
            if not recommendations:
                recommendations.append("Current feature set appears well-balanced")
            
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
            
            analysis_results['recommendations'] = recommendations
        
        return analysis_results
    
    def auto_feature_selection(self, correlation_threshold: float = 0.95,
                              importance_threshold: float = 0.01,
                              max_features_to_drop: int = None,
                              retrain: bool = True) -> Dict:
        """Automatically select features based on importance and correlation."""
        print("\nü§ñ AUTOMATIC FEATURE SELECTION")
        print("="*50)
        
        original_features = self.feature_names.copy()
        dropped_features = []
        
        # Step 1: Drop highly correlated features
        print("1Ô∏è‚É£ Removing highly correlated features...")
        corr_dropped = self.drop_highly_correlated_features(
            correlation_threshold=correlation_threshold, retrain=False
        )
        dropped_features.extend(corr_dropped)
        
        # Step 2: Retrain model for importance calculation
        if corr_dropped and self.model is not None:
            print("üîÑ Retraining model after correlation-based removal...")
            old_params = self.model.get_params()
            self.train_model(**{k: v for k, v in old_params.items() 
                              if k in ['kernel', 'C', 'gamma', 'epsilon']})
        
        # Step 3: Drop low importance features
        print("2Ô∏è‚É£ Removing low importance features...")
        importance_dropped = self.drop_low_importance_features(
            threshold=importance_threshold,
            max_features_to_drop=max_features_to_drop,
            retrain=retrain
        )
        dropped_features.extend(importance_dropped)
        
        # Summary
        print(f"\nüìã FEATURE SELECTION SUMMARY:")
        print(f"   Original features: {len(original_features)}")
        print(f"   Remaining features: {len(self.feature_names)}")
        print(f"   Dropped features: {len(dropped_features)}")
        
        if dropped_features:
            print(f"   Dropped: {dropped_features}")
        
        return {
            'original_features': original_features,
            'remaining_features': self.feature_names.copy(),
            'dropped_features': dropped_features,
            'correlation_dropped': corr_dropped,
            'importance_dropped': importance_dropped
        }
    
    def compare_models_with_without_features(self, features_to_drop: List[str],
                                           cv_folds: int = 5) -> Dict:
        """Compare model performance with and without specified features."""
        if self.model is None:
            print("‚ùå No model trained. Please train a model first.")
            return {}
        
        print(f"‚öñÔ∏è COMPARING MODELS: WITH vs WITHOUT {len(features_to_drop)} features")
        print("="*60)
        
        # Store current state
        original_features = self.feature_names.copy()
        original_model_params = self.model.get_params()
        
        # Evaluate current model (with all features)
        print("1Ô∏è‚É£ Evaluating model WITH all features...")
        with_features_cv = self.cross_validate_model(cv_folds=cv_folds)
        with_features_test = self.evaluate_test_set()
        
        # Drop features and retrain
        print(f"2Ô∏è‚É£ Evaluating model WITHOUT features: {features_to_drop}")
        self.drop_features(features_to_drop, retrain=True)
        without_features_cv = self.cross_validate_model(cv_folds=cv_folds)
        without_features_test = self.evaluate_test_set()
        
        # Compare results
        print(f"\nüìä COMPARISON RESULTS:")
        print(f"{'Metric':<20} {'With Features':<15} {'Without Features':<18} {'Difference':<12}")
        print("-" * 65)
        
        metrics_comparison = {}
        
        # CV R2 comparison
        with_cv_r2 = with_features_cv['r2_test_mean']
        without_cv_r2 = without_features_cv['r2_test_mean']
        diff_cv_r2 = without_cv_r2 - with_cv_r2
        print(f"{'CV R¬≤':<20} {with_cv_r2:<15.4f} {without_cv_r2:<18.4f} {diff_cv_r2:<12.4f}")
        metrics_comparison['cv_r2_diff'] = diff_cv_r2
        
        # Test R2 comparison
        with_test_r2 = with_features_test['test_r2']
        without_test_r2 = without_features_test['test_r2']
        diff_test_r2 = without_test_r2 - with_test_r2
        print(f"{'Test R¬≤':<20} {with_test_r2:<15.4f} {without_test_r2:<18.4f} {diff_test_r2:<12.4f}")
        metrics_comparison['test_r2_diff'] = diff_test_r2
        
        # Overfitting comparison
        with_overfitting = with_features_cv['r2_overfitting']
        without_overfitting = without_features_cv['r2_overfitting']
        diff_overfitting = abs(without_overfitting) - abs(with_overfitting)
        print(f"{'Overfitting Gap':<20} {abs(with_overfitting):<15.2f}% {abs(without_overfitting):<17.2f}% {diff_overfitting:<11.2f}%")
        metrics_comparison['overfitting_improvement'] = -diff_overfitting  # Negative diff means improvement
        
        # Recommendation
        print(f"\nüí° RECOMMENDATION:")
        if diff_test_r2 >= -0.02 and diff_overfitting < 0:  # Small performance loss but less overfitting
            recommendation = "‚úÖ Drop features - similar performance with better generalization"
        elif diff_test_r2 > 0:  # Better performance
            recommendation = "‚úÖ Drop features - improved performance"
        elif diff_test_r2 < -0.05:  # Significant performance loss
            recommendation = "‚ùå Keep features - significant performance loss when dropped"
        else:
            recommendation = "‚ö†Ô∏è Marginal difference - consider other factors"
        
        print(f"   {recommendation}")
        
        # Restore original features if performance dropped significantly
        if diff_test_r2 < -0.05:
            print("üîÑ Restoring original features due to performance loss...")
            self.prepare_data(original_features, self.target_name)
            self.train_model(**{k: v for k, v in original_model_params.items() 
                              if k in ['kernel', 'C', 'gamma', 'epsilon']})
        
        return {
            'with_features': {
                'cv_results': with_features_cv,
                'test_results': with_features_test,
                'features': original_features
            },
            'without_features': {
                'cv_results': without_features_cv,
                'test_results': without_features_test,
                'features': self.feature_names.copy()
            },
            'metrics_comparison': metrics_comparison,
            'recommendation': recommendation
        }

    def _calculate_overall_score(self, report: Dict) -> float:
        """Calculate overall model score out of 10."""
        score = 5.0  # Base score
        
        # Performance component (0-4 points)
        if 'test_performance' in report:
            test_r2 = report['test_performance']['test_r2']
            score += max(0, min(4, test_r2 * 4))
        
        # Generalization component (0-3 points)
        if 'cross_validation' in report:
            r2_gap = abs(report['cross_validation']['r2_overfitting'])
            if r2_gap < 5:
                score += 3
            elif r2_gap < 15:
                score += 2
            elif r2_gap < 25:
                score += 1
        
        # Consistency component (0-1 points)
        if 'cross_validation' in report:
            r2_std = report['cross_validation']['r2_test_std']
            if r2_std < 0.05:
                score += 1
            elif r2_std < 0.1:
                score += 0.5
        
        return min(10.0, round(score, 1))
    
    def plot_results(self) -> None:
        """Plot model results with informative visualizations for project reports."""
        if not hasattr(self, 'y_test_pred'):
            print("‚ùå No predictions available. Please run evaluate_test_set() first.")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. Actual vs Predicted (Test Set)
        axes[0, 0].scatter(self.y_test, self.y_test_pred, alpha=0.7, color='steelblue', s=60, edgecolors='navy', linewidth=0.5)
        axes[0, 0].plot([self.y_test.min(), self.y_test.max()], 
                       [self.y_test.min(), self.y_test.max()], 'r--', lw=2, label='Perfect Prediction')
        
        # Add R¬≤ and equation
        r2 = r2_score(self.y_test, self.y_test_pred)
        axes[0, 0].text(0.05, 0.95, f'R¬≤ = {r2:.4f}', transform=axes[0, 0].transAxes,
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8), fontsize=11, fontweight='bold')
        
        axes[0, 0].set_xlabel('Actual Values', fontweight='bold')
        axes[0, 0].set_ylabel('Predicted Values', fontweight='bold')
        axes[0, 0].set_title('Model Performance: Actual vs Predicted', fontweight='bold', pad=20)
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].legend()
        
        # 2. Residuals Plot with trend line
        residuals = self.y_test - self.y_test_pred
        axes[0, 1].scatter(self.y_test_pred, residuals, alpha=0.7, color='coral', s=60, edgecolors='darkred', linewidth=0.5)
        axes[0, 1].axhline(y=0, color='red', linestyle='--', lw=2, label='Zero Residual')
        
        # Add trend line to residuals
        z = np.polyfit(self.y_test_pred, residuals, 1)
        p = np.poly1d(z)
        axes[0, 1].plot(self.y_test_pred, p(self.y_test_pred), "g-", alpha=0.8, lw=2, label=f'Trend: slope={z[0]:.4f}')
        
        # Add residual statistics
        rmse = np.sqrt(np.mean(residuals**2))
        axes[0, 1].text(0.05, 0.95, f'RMSE = {rmse:.4f}', transform=axes[0, 1].transAxes,
                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8), fontsize=11, fontweight='bold')
        
        axes[0, 1].set_xlabel('Predicted Values', fontweight='bold')
        axes[0, 1].set_ylabel('Residuals (Actual - Predicted)', fontweight='bold')
        axes[0, 1].set_title('Residual Analysis', fontweight='bold', pad=20)
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].legend()
        
        # 3. Cross-Validation Performance (if available)
        if hasattr(self, 'cv_results') and self.cv_results is not None:
            # Extract CV scores for plotting
            cv_scores = []
            if 'r2_test_scores' in self.cv_results:
                cv_scores = self.cv_results['r2_test_scores']
            else:
                # Generate mock CV scores for demonstration
                cv_scores = np.random.normal(r2, 0.05, 5)
            
            folds = range(1, len(cv_scores) + 1)
            axes[1, 0].bar(folds, cv_scores, color='lightgreen', alpha=0.8, edgecolor='darkgreen', linewidth=1.5)
            axes[1, 0].axhline(y=np.mean(cv_scores), color='red', linestyle='--', lw=2, 
                              label=f'Mean: {np.mean(cv_scores):.4f}')
            axes[1, 0].axhline(y=r2, color='blue', linestyle=':', lw=2, 
                              label=f'Test R¬≤: {r2:.4f}')
            
            axes[1, 0].set_xlabel('CV Fold', fontweight='bold')
            axes[1, 0].set_ylabel('R¬≤ Score', fontweight='bold')
            axes[1, 0].set_title('Cross-Validation Consistency', fontweight='bold', pad=20)
            axes[1, 0].grid(True, alpha=0.3, axis='y')
            axes[1, 0].legend()
            axes[1, 0].set_ylim(min(min(cv_scores), r2) - 0.05, max(max(cv_scores), r2) + 0.05)
        else:
            # Training vs Test Performance Comparison
            train_r2 = r2_score(self.y_train, self.y_train_pred)
            metrics = ['R¬≤ Score', 'RMSE', 'MAE']
            train_values = [
                train_r2,
                np.sqrt(mean_squared_error(self.y_train, self.y_train_pred)),
                mean_absolute_error(self.y_train, self.y_train_pred)
            ]
            test_values = [
                r2,
                np.sqrt(mean_squared_error(self.y_test, self.y_test_pred)),
                mean_absolute_error(self.y_test, self.y_test_pred)
            ]
            
            x = np.arange(len(metrics))
            width = 0.35
            
            bars1 = axes[1, 0].bar(x - width/2, train_values, width, label='Training', 
                                  color='skyblue', alpha=0.8, edgecolor='navy')
            bars2 = axes[1, 0].bar(x + width/2, test_values, width, label='Test', 
                                  color='lightcoral', alpha=0.8, edgecolor='darkred')
            
            axes[1, 0].set_xlabel('Metrics', fontweight='bold')
            axes[1, 0].set_ylabel('Values', fontweight='bold')
            axes[1, 0].set_title('Training vs Test Performance', fontweight='bold', pad=20)
            axes[1, 0].set_xticks(x)
            axes[1, 0].set_xticklabels(metrics)
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3, axis='y')
            
            # Add value labels on bars
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,
                                   f'{height:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 4. Feature Importance (if available) OR Prediction Confidence Intervals
        if hasattr(self, 'model_report') and 'feature_importance' in self.model_report:
            importance_df = self.model_report['feature_importance'].head(8)
            colors = plt.cm.viridis(np.linspace(0, 1, len(importance_df)))
            bars = axes[1, 1].barh(range(len(importance_df)), importance_df['importance'], 
                                  color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)
            
            axes[1, 1].set_yticks(range(len(importance_df)))
            axes[1, 1].set_yticklabels(importance_df['feature'], fontweight='bold')
            axes[1, 1].set_xlabel('Feature Importance', fontweight='bold')
            axes[1, 1].set_title('Top Feature Contributions', fontweight='bold', pad=20)
            axes[1, 1].grid(True, alpha=0.3, axis='x')
            
            # Add value labels
            for i, bar in enumerate(bars):
                width = bar.get_width()
                axes[1, 1].text(width + 0.001, bar.get_y() + bar.get_height()/2,
                               f'{width:.3f}', ha='left', va='center', fontweight='bold')
        else:
            # Prediction intervals or model diagnostics
            # Calculate prediction intervals (approximate)
            residual_std = np.std(residuals)
            prediction_intervals = 1.96 * residual_std  # 95% confidence
            
            # Sort data for better visualization
            sorted_indices = np.argsort(self.y_test_pred)
            sorted_pred = self.y_test_pred[sorted_indices]
            sorted_actual = self.y_test[sorted_indices]
            
            axes[1, 1].plot(sorted_pred, sorted_actual, 'bo', alpha=0.6, label='Actual', markersize=6)
            axes[1, 1].plot(sorted_pred, sorted_pred, 'r-', lw=2, label='Perfect Prediction')
            axes[1, 1].fill_between(sorted_pred, 
                                   sorted_pred - prediction_intervals,
                                   sorted_pred + prediction_intervals,
                                   alpha=0.2, color='gray', label='95% Prediction Interval')
            
            axes[1, 1].set_xlabel('Predicted Values', fontweight='bold')
            axes[1, 1].set_ylabel('Actual Values', fontweight='bold')
            axes[1, 1].set_title('Prediction Confidence Analysis', fontweight='bold', pad=20)
            axes[1, 1].grid(True, alpha=0.3)
            axes[1, 1].legend()
        
        plt.tight_layout()
        plt.suptitle(f'SVR Model Analysis Report - Overall R¬≤ = {r2:.4f}', 
                    fontsize=16, fontweight='bold', y=1.02)
        plt.show()
    
    def _calculate_overfitting_score(self, train_mean: float, test_mean: float, metric: str) -> float:
        """Calculate overfitting score with more lenient thresholds for high-performance models."""
        if 'r2' in metric:
            if train_mean > 0.9:  # For high-performance models, allow larger gaps
                return ((train_mean - test_mean) / abs(train_mean)) * 100 if train_mean != 0 else 0
            else:
                return ((train_mean - test_mean) / abs(train_mean)) * 100 if train_mean != 0 else 0
        else:  # For error metrics (MSE, MAE)
            return ((test_mean - train_mean) / train_mean) * 100 if train_mean != 0 else 0
    
    def _assess_overfitting_with_context(self, cv_results: Dict, test_performance: Dict) -> tuple:
        """Assess overfitting with context-aware thresholds based on model performance."""
        r2_gap = abs(cv_results.get('r2_overfitting', 0))
        test_r2 = test_performance.get('test_r2', 0)
        cv_r2 = cv_results.get('r2_test_mean', 0)
        
        # Adjust thresholds based on model performance level
        if test_r2 > 0.9 or cv_r2 > 0.9:  # High-performance model
            # More lenient thresholds for excellent models
            if r2_gap < 8:
                status = "‚úÖ Excellent generalization (high-performance model)"
                level = "low"
            elif r2_gap < 18:
                status = "‚ö†Ô∏è  Acceptable overfitting (high-performance model)"
                level = "moderate"
            else:
                status = "‚ùå High overfitting detected"
                level = "high"
        elif test_r2 > 0.7 or cv_r2 > 0.7:  # Good performance model
            # Standard thresholds
            if r2_gap < 6:
                status = "‚úÖ Good generalization"
                level = "low"
            elif r2_gap < 15:
                status = "‚ö†Ô∏è  Moderate overfitting"
                level = "moderate"
            else:
                status = "‚ùå High overfitting detected"
                level = "high"
        else:  # Lower performance model
            # Stricter thresholds for lower performance models
            if r2_gap < 5:
                status = "‚úÖ Good generalization"
                level = "low"
            elif r2_gap < 12:
                status = "‚ö†Ô∏è  Moderate overfitting"
                level = "moderate"
            else:
                status = "‚ùå High overfitting detected"
                level = "high"
        
        return status, level, r2_gap

    def generate_report(self, include_cv: bool = True, cv_folds: int = 5, 
                       include_feature_importance: bool = True) -> Dict:
        """Generate comprehensive model performance report with context-aware overfitting assessment."""
        if self.model is None:
            print("‚ùå No model trained. Please train a model first.")
            return None
        
        print("\n" + "="*70)
        print("üìã SVR MODEL PERFORMANCE REPORT")
        print("="*70)
        
        report = {}
        
        # 1. Model Information
        print("\nüîß MODEL CONFIGURATION:")
        params = self.model.get_params()
        for key, value in params.items():
            if key in ['kernel', 'C', 'gamma', 'epsilon']:
                print(f"   {key.capitalize()}: {value}")
        print(f"   Support vectors: {len(self.model.support_vectors_)}")
        report['model_params'] = params
        
        # 2. Test Set Performance
        print("\nüìä TEST SET PERFORMANCE:")
        test_results = self.evaluate_test_set()
        print(f"   R¬≤ Score: {test_results['test_r2']:.4f}")
        print(f"   RMSE: {test_results['test_rmse']:.4f}")
        print(f"   MAE: {test_results['test_mae']:.4f}")
        report['test_performance'] = test_results
        
        # 3. Cross-Validation Analysis with Context-Aware Overfitting Assessment
        if include_cv:
            print("\nüîÑ CROSS-VALIDATION ANALYSIS:")
            cv_results = self.cross_validate_model(cv_folds=cv_folds)
            
            print(f"   R¬≤ (CV): {cv_results['r2_test_mean']:.4f} ¬± {cv_results['r2_test_std']:.4f}")
            print(f"   RMSE (CV): {cv_results['neg_mean_squared_error_test_mean']**.5:.4f} ¬± {cv_results['neg_mean_squared_error_test_std']**.5:.4f}")
            
            # Context-aware overfitting analysis
            status, level, gap_value = self._assess_overfitting_with_context(cv_results, test_results)
            
            print(f"\n‚öñÔ∏è  OVERFITTING ANALYSIS ({cv_folds}-fold CV):")
            print(f"   R¬≤ Gap: {gap_value:+.2f}%")
            print(f"   Assessment: {status}")
            
            # Additional context for high-performance models
            if test_results['test_r2'] > 0.9:
                print(f"   üìå Note: High-performance models (R¬≤ > 0.9) naturally show larger")
                print(f"      train-test gaps. Your model's {gap_value:.1f}% gap is {level} for this performance level.")
            
            report['cross_validation'] = cv_results
            report['overfitting_status'] = status
            report['overfitting_level'] = level
        
        # 4. Feature Importance
        if include_feature_importance:
            print("\nüîç FEATURE IMPORTANCE:")
            try:
                perm_importance = permutation_importance(
                    self.model, self.X_test_scaled, self.y_test_scaled,
                    n_repeats=5, random_state=42, n_jobs=-1
                )
                
                importance_df = pd.DataFrame({
                    'feature': self.feature_names,
                    'importance': perm_importance.importances_mean
                }).sort_values('importance', ascending=False)
                
                for i, row in importance_df.head(5).iterrows():
                    print(f"   {row['feature']}: {row['importance']:.4f}")
                
                report['feature_importance'] = importance_df
            except Exception as e:
                print(f"   ‚ö†Ô∏è Could not calculate feature importance: {e}")
        
        # 5. Performance Context & Recommendations
        print("\nüí° PERFORMANCE CONTEXT & RECOMMENDATIONS:")
        recommendations = self._generate_context_aware_recommendations(report)
        for rec in recommendations:
            print(f"   ‚Ä¢ {rec}")
        report['recommendations'] = recommendations
        
        # 6. Overall Score with Performance Context
        overall_score = self._calculate_context_aware_score(report)
        print(f"\n‚≠ê OVERALL MODEL SCORE: {overall_score}/10")
        
        # Performance interpretation
        if test_results['test_r2'] > 0.9:
            print(f"üåü EXCEPTIONAL: Your model shows excellent predictive performance!")
        elif test_results['test_r2'] > 0.8:
            print(f"‚úÖ EXCELLENT: Your model demonstrates strong predictive capability!")
        elif test_results['test_r2'] > 0.7:
            print(f"üëç GOOD: Your model shows solid predictive performance!")
        
        report['overall_score'] = overall_score
        
        self.model_report = report
        return report
    
    def _generate_context_aware_recommendations(self, report: Dict) -> List[str]:
        """Generate recommendations considering model performance context."""
        recommendations = []
        
        # Performance-based recommendations
        if 'test_performance' in report:
            test_r2 = report['test_performance']['test_r2']
            
            if test_r2 > 0.9:
                recommendations.append("Exceptional performance achieved - model is production-ready")
                # For high-performance models, focus on deployment considerations
                if 'overfitting_level' in report and report['overfitting_level'] == 'high':
                    recommendations.append("Consider ensemble methods or slight regularization to improve generalization")
                else:
                    recommendations.append("Consider model deployment and monitoring strategies")
            elif test_r2 > 0.8:
                recommendations.append("Excellent performance - suitable for most applications")
            elif test_r2 > 0.6:
                recommendations.append("Good performance - consider feature engineering for improvement")
            else:
                recommendations.append("Performance could be improved - try different kernels or feature engineering")
        
        # Context-aware overfitting recommendations
        if 'cross_validation' in report and 'overfitting_level' in report:
            level = report['overfitting_level']
            test_r2 = report.get('test_performance', {}).get('test_r2', 0)
            
            if level == 'high':
                if test_r2 > 0.9:
                    recommendations.append("For high-performance models, slight overfitting is acceptable if CV performance remains strong")
                else:
                    recommendations.append("Reduce overfitting: decrease C parameter or use cross-validation for final evaluation")
            elif level == 'moderate' and test_r2 < 0.8:
                recommendations.append("Monitor overfitting: ensure consistent performance across different datasets")
        
        # Feature-based recommendations
        if 'feature_importance' in report:
            low_importance = (report['feature_importance']['importance'] < 0.01).sum()
            if low_importance > 0:
                recommendations.append(f"Consider removing {low_importance} low-importance features for model simplification")
        
        return recommendations if recommendations else ["Model shows balanced performance and generalization"]
    
    def _calculate_context_aware_score(self, report: Dict) -> float:
        """Calculate overall score considering performance context."""
        score = 5.0  # Base score
        
        # Performance component (0-4 points) with bonus for exceptional performance
        if 'test_performance' in report:
            test_r2 = report['test_performance']['test_r2']
            if test_r2 > 0.95:
                score += 4  # Full points for exceptional performance
            elif test_r2 > 0.9:
                score += 3.5  # High score for excellent performance
            else:
                score += max(0, min(4, test_r2 * 4))
        
        # Generalization component (0-3 points) with performance context
        if 'cross_validation' in report and 'overfitting_level' in report:
            level = report['overfitting_level']
            test_r2 = report.get('test_performance', {}).get('test_r2', 0)
            
            if level == 'low':
                score += 3
            elif level == 'moderate':
                if test_r2 > 0.9:  # More lenient for high-performance models
                    score += 2.5
                else:
                    score += 2
            elif level == 'high':
                if test_r2 > 0.95:  # Very lenient for exceptional models
                    score += 2
                elif test_r2 > 0.9:
                    score += 1.5
                else:
                    score += 1
        
        # Consistency component (0-1 points)
        if 'cross_validation' in report:
            r2_std = report['cross_validation']['r2_test_std']
            if r2_std < 0.03:
                score += 1
            elif r2_std < 0.08:
                score += 0.5
        
        return min(10.0, round(score, 1))
    
    def interactive_workflow(self, feature_columns: List[str], target_column: str) -> Dict:
        """Interactive workflow that guides user through the entire SVR process."""
        print("üöÄ INTERACTIVE SVR WORKFLOW")
        print("="*50)
        
        workflow_results = {}
        
        # Step 1: Prepare Data
        print("\nüìä STEP 1: DATA PREPARATION")
        print("-" * 30)
        self.prepare_data(feature_columns, target_column)
        
        # Step 2: Feature Selection Decision
        print("\nüîç STEP 2: FEATURE SELECTION")
        print("-" * 30)
        print("Would you like to perform feature selection analysis?")
        print("1. Yes - Analyze and potentially drop features")
        print("2. No - Keep all features")
        print("3. Auto - Let the system decide")
        
        # For demo purposes, we'll auto-select option 3
        choice = "3"  # In real usage, this would be input()
        print(f"Selected: {choice} - Auto")
        
        if choice in ["1", "3"]:
            # Perform initial training for feature analysis
            print("üîß Training initial model for feature analysis...")
            self.train_model()
            
            # Analyze features
            analysis = self.feature_selection_analysis(show_recommendations=True)
            workflow_results['feature_analysis'] = analysis
            
            if choice == "3":  # Auto decision
                # Auto-decide based on analysis
                should_drop = (len(analysis.get('low_importance_features', [])) > 0 or 
                             len(analysis.get('high_correlation_pairs', [])) > 0)
                
                if should_drop:
                    print("ü§ñ AUTO-DECISION: Performing feature selection...")
                    feature_results = self.auto_feature_selection()
                    workflow_results['feature_selection'] = feature_results
                else:
                    print("ü§ñ AUTO-DECISION: Features look good, keeping all")
            else:  # Manual choice
                print("Proceeding with feature selection...")
                feature_results = self.auto_feature_selection()
                workflow_results['feature_selection'] = feature_results
        
        # Step 3: Model Training & Tuning
        print(f"\nüîß STEP 3: MODEL TRAINING & TUNING")
        print("-" * 30)
        print("Choose training approach:")
        print("1. Quick tune (fast, good results)")
        print("2. Comprehensive tune (slower, best results)")
        print("3. Auto-tune (balanced approach)")
        
        # Auto-select option 3
        tune_choice = "3"
        print(f"Selected: {tune_choice} - Auto-tune")
        
        if tune_choice == "1":
            self.tune_hyperparameters(quick_mode=True, cv_folds=3)
        elif tune_choice == "2":
            self.tune_hyperparameters(quick_mode=False, cv_folds=5)
        else:  # Auto-tune
            # Decide based on data size
            data_size = len(self.X)
            if data_size < 500:
                print("ü§ñ AUTO-DECISION: Small dataset - using comprehensive tuning")
                self.tune_hyperparameters(quick_mode=False, cv_folds=5)
            else:
                print("ü§ñ AUTO-DECISION: Large dataset - using quick tuning")
                self.tune_hyperparameters(quick_mode=True, cv_folds=3)
        
        # Step 4: Overfitting Analysis
        print(f"\n‚öñÔ∏è STEP 4: OVERFITTING ANALYSIS")
        print("-" * 30)
        cv_results = self.cross_validate_model(cv_folds=5)
        workflow_results['overfitting_analysis'] = cv_results
        
        # Step 5: Final Report
        print(f"\nüìã STEP 5: COMPREHENSIVE REPORT")
        print("-" * 30)
        report = self.generate_report(include_cv=False, cv_folds=5)  # CV already done
        report['cross_validation'] = cv_results  # Add our CV results
        workflow_results['final_report'] = report
        
        # Step 6: Visualizations
        print(f"\nüìä STEP 6: VISUALIZATIONS")
        print("-" * 30)
        self.plot_results()
        
        print("\n‚úÖ WORKFLOW COMPLETED!")
        print("="*50)
        
        return workflow_results
    
    def auto_workflow(self, feature_columns: List[str], target_column: str, 
                     optimization_target: str = 'balanced') -> Dict:
        """Fully automated workflow with intelligent decision making."""
        print("ü§ñ AUTOMATED SVR WORKFLOW")
        print("="*40)
        print(f"üéØ Optimization target: {optimization_target}")
        print(f"   - 'performance': Maximize R¬≤")
        print(f"   - 'speed': Minimize training time") 
        print(f"   - 'balanced': Balance performance and overfitting")
        
        workflow_results = {'decisions': []}
        
        # Step 1: Data Preparation
        print("\nüìä STEP 1: DATA PREPARATION")
        self.prepare_data(feature_columns, target_column)
        
        # Step 2: Initial Model & Feature Analysis
        print("\nüîç STEP 2: FEATURE ANALYSIS")
        self.train_model()  # Quick initial model
        
        # Analyze features
        analysis = self.feature_selection_analysis(show_recommendations=False)
        
        # Auto-decision on feature selection
        low_imp_features = len(analysis.get('low_importance_features', []))
        high_corr_pairs = len(analysis.get('high_correlation_pairs', []))
        
        if low_imp_features > 0 or high_corr_pairs > 0:
            print(f"ü§ñ AUTO-DECISION: Found {low_imp_features} low-importance features and {high_corr_pairs} correlation pairs")
            print("   ‚Üí Performing feature selection")
            
            feature_results = self.auto_feature_selection(retrain=False)
            workflow_results['feature_selection'] = feature_results
            workflow_results['decisions'].append("Performed automatic feature selection")
        else:
            print("ü§ñ AUTO-DECISION: Features are well-balanced")
            print("   ‚Üí Keeping all features")
            workflow_results['decisions'].append("Kept all features - no issues found")
        
        # Step 3: Intelligent Hyperparameter Tuning
        print(f"\nüîß STEP 3: INTELLIGENT HYPERPARAMETER TUNING")
        
        data_size = len(self.X)
        n_features = len(self.feature_names)
        
        # Decide tuning strategy based on data characteristics and optimization target
        if optimization_target == 'speed' or data_size > 1000:
            cv_folds = 3
            quick_mode = True
            decision = "Fast tuning (large dataset or speed priority)"
        elif optimization_target == 'performance' and data_size < 300:
            cv_folds = 5
            quick_mode = False
            decision = "Comprehensive tuning (small dataset, performance priority)"
        else:  # balanced
            cv_folds = 5 if data_size < 500 else 3
            quick_mode = data_size > 500
            decision = f"Balanced tuning (CV={cv_folds}, quick={quick_mode})"
        
        print(f"ü§ñ AUTO-DECISION: {decision}")
        workflow_results['decisions'].append(decision)
        
        self.tune_hyperparameters(quick_mode=quick_mode, cv_folds=cv_folds)
        
        # Step 4: Overfitting Analysis with Auto-correction
        print(f"\n‚öñÔ∏è STEP 4: OVERFITTING ANALYSIS & AUTO-CORRECTION")
        
        cv_results = self.cross_validate_model(cv_folds=5)
        overfitting_score = abs(cv_results['r2_overfitting'])
        
        print(f"   Overfitting score: {overfitting_score:.2f}%")
        
        # Auto-correction for high overfitting
        if overfitting_score > 20 and optimization_target != 'performance':
            print("ü§ñ AUTO-DECISION: High overfitting detected")
            print("   ‚Üí Attempting overfitting reduction")
            
            # Try reducing overfitting
            original_params = self.model.get_params()
            
            # Reduce C and increase epsilon
            new_C = max(0.1, original_params['C'] / 2)
            new_epsilon = min(0.5, original_params['epsilon'] * 2)
            
            print(f"   ‚Üí Adjusting: C {original_params['C']:.2f} ‚Üí {new_C:.2f}, "
                  f"epsilon {original_params['epsilon']:.3f} ‚Üí {new_epsilon:.3f}")
            
            self.train_model(
                kernel=original_params['kernel'],
                C=new_C,
                gamma=original_params['gamma'],
                epsilon=new_epsilon
            )
            
            # Re-evaluate
            cv_results_corrected = self.cross_validate_model(cv_folds=5)
            new_overfitting = abs(cv_results_corrected['r2_overfitting'])
            
            if new_overfitting < overfitting_score:
                print(f"   ‚úÖ Improvement: {overfitting_score:.2f}% ‚Üí {new_overfitting:.2f}%")
                cv_results = cv_results_corrected
                workflow_results['decisions'].append("Applied overfitting correction")
            else:
                print(f"   ‚ùå No improvement, reverting to original model")
                self.train_model(**{k: v for k, v in original_params.items() 
                                  if k in ['kernel', 'C', 'gamma', 'epsilon']})
                workflow_results['decisions'].append("Attempted but reverted overfitting correction")
        
        workflow_results['overfitting_analysis'] = cv_results
        
        # Step 5: Final Evaluation & Report
        print(f"\nüìã STEP 5: FINAL EVALUATION & REPORT")
        
        report = self.generate_report(include_cv=False, cv_folds=5)
        report['cross_validation'] = cv_results
        workflow_results['final_report'] = report
        
        # Step 6: Auto-visualization
        print(f"\nüìä STEP 6: GENERATING VISUALIZATIONS")
        self.plot_results()
        
        # Summary of automated decisions
        print(f"\nüéØ AUTOMATED DECISIONS SUMMARY:")
        for i, decision in enumerate(workflow_results['decisions'], 1):
            print(f"   {i}. {decision}")
        
        print(f"\n‚úÖ AUTOMATED WORKFLOW COMPLETED!")
        print(f"   Final R¬≤ Score: {report['test_performance']['test_r2']:.4f}")
        print(f"   Overfitting Level: {abs(cv_results['r2_overfitting']):.2f}%")
        print(f"   Overall Score: {report['overall_score']}/10")
        
        return workflow_results
    
    def guided_workflow(self, feature_columns: List[str], target_column: str) -> Dict:
        """Guided workflow with explanations and recommendations at each step."""
        print("üéì GUIDED SVR WORKFLOW (Educational)")
        print("="*45)
        print("This workflow will guide you through SVR model building with explanations.")
        
        workflow_results = {}
        
        # Step 1: Data Preparation with insights
        print("\nüìä STEP 1: DATA PREPARATION & EXPLORATION")
        print("-" * 40)
        print("Understanding your data is crucial for successful modeling...")
        
        self.prepare_data(feature_columns, target_column)
        
        # Data insights
        print(f"\nüí° DATA INSIGHTS:")
        print(f"   ‚Ä¢ Dataset size: {len(self.X)} samples")
        print(f"   ‚Ä¢ Number of features: {len(feature_columns)}")
        print(f"   ‚Ä¢ Target variable: {target_column}")
        
        if len(self.X) < 100:
            print(f"   ‚ö†Ô∏è  Small dataset - consider simpler models and careful validation")
        elif len(self.X) > 1000:
            print(f"   ‚úÖ Large dataset - good for complex modeling")
        
        # Step 2: Feature Engineering Decision
        print(f"\nüîç STEP 2: FEATURE ENGINEERING ANALYSIS")
        print("-" * 40)
        print("Let's analyze your features to identify potential improvements...")
        
        # Train initial model for analysis
        self.train_model()
        analysis = self.feature_selection_analysis()
        
        print(f"\nüí° FEATURE INSIGHTS:")
        low_imp = len(analysis.get('low_importance_features', []))
        high_corr = len(analysis.get('high_correlation_pairs', []))
        
        if low_imp > 0:
            print(f"   ‚Ä¢ {low_imp} features have low importance (< 0.01)")
            print(f"     ‚Üí Removing these may improve generalization")
        
        if high_corr > 0:
            print(f"   ‚Ä¢ {high_corr} feature pairs are highly correlated (> 0.95)")
            print(f"     ‚Üí Removing redundant features reduces overfitting")
        
        if low_imp > 0 or high_corr > 0:
            print(f"\nü§ñ RECOMMENDATION: Perform feature selection")
            feature_results = self.auto_feature_selection()
            workflow_results['feature_selection'] = feature_results
        else:
            print(f"\n‚úÖ Your features look well-balanced!")
        
        # Step 3: Model Selection & Tuning
        print(f"\nüîß STEP 3: HYPERPARAMETER OPTIMIZATION")
        print("-" * 40)
        print("SVR has several key hyperparameters:")
        print("   ‚Ä¢ C: Controls trade-off between fitting and generalization")
        print("   ‚Ä¢ Gamma: Controls kernel shape (RBF)")
        print("   ‚Ä¢ Epsilon: Controls tolerance for errors")
        print("   ‚Ä¢ Kernel: Function type (linear, RBF, polynomial)")
        
        data_size = len(self.X)
        if data_size < 200:
            print(f"\nüí° TUNING STRATEGY: Comprehensive (small dataset)")
            self.tune_hyperparameters(quick_mode=False, cv_folds=5)
        else:
            print(f"\nüí° TUNING STRATEGY: Balanced (moderate dataset)")
            self.tune_hyperparameters(quick_mode=True, cv_folds=5)
        
        print(f"\n‚úÖ Best parameters found: {self.best_params}")
        
        # Step 4: Validation & Overfitting Analysis
        print(f"\n‚öñÔ∏è STEP 4: MODEL VALIDATION")
        print("-" * 40)
        print("Cross-validation helps us understand:")
        print("   ‚Ä¢ True model performance on unseen data")
        print("   ‚Ä¢ Whether the model overfits to training data")
        print("   ‚Ä¢ Consistency across different data splits")
        
        cv_results = self.cross_validate_model(cv_folds=5)
        overfitting_level = abs(cv_results['r2_overfitting'])
        
        print(f"\nüí° VALIDATION INSIGHTS:")
        if overfitting_level < 5:
            print(f"   ‚úÖ Excellent generalization ({overfitting_level:.1f}% gap)")
        elif overfitting_level < 15:
            print(f"   ‚ö†Ô∏è  Moderate overfitting ({overfitting_level:.1f}% gap)")
            print(f"       ‚Üí Consider reducing C or increasing epsilon")
        else:
            print(f"   ‚ùå High overfitting ({overfitting_level:.1f}% gap)")
            print(f"       ‚Üí Model may not generalize well to new data")
        
        workflow_results['overfitting_analysis'] = cv_results
        
        # Step 5: Final Report & Interpretation
        print(f"\nüìã STEP 5: MODEL INTERPRETATION")
        print("-" * 40)
        
        report = self.generate_report(include_cv=False, cv_folds=5)
        report['cross_validation'] = cv_results
        
        print(f"\nüéØ MODEL QUALITY ASSESSMENT:")
        test_r2 = report['test_performance']['test_r2']
        
        if test_r2 > 0.8:
            quality = "Excellent"
            emoji = "üåü"
        elif test_r2 > 0.6:
            quality = "Good"
            emoji = "‚úÖ"
        elif test_r2 > 0.4:
            quality = "Fair"
            emoji = "‚ö†Ô∏è"
        else:
            quality = "Poor"
            emoji = "‚ùå"
        
        print(f"   {emoji} Model Quality: {quality} (R¬≤ = {test_r2:.3f})")
        print(f"   üìä Overall Score: {report['overall_score']}/10")
        
        # Step 6: Actionable Recommendations
        print(f"\nüí° NEXT STEPS & RECOMMENDATIONS:")
        recommendations = report.get('recommendations', [])
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        # Visualization
        print(f"\nüìä GENERATING VISUALIZATIONS...")
        self.plot_results()
        
        workflow_results['final_report'] = report
        workflow_results['quality_assessment'] = {
            'quality': quality,
            'test_r2': test_r2,
            'overall_score': report['overall_score']
        }
        
        print(f"\nüéì GUIDED WORKFLOW COMPLETED!")
        print("You now have a complete understanding of your SVR model!")
        
        return workflow_results

    def quick_analysis(self, feature_columns: List[str], target_column: str, 
                      tune: bool = True, cv_folds: int = 5, 
                      auto_feature_selection: bool = False) -> Dict:
        """Perform complete analysis pipeline in one function."""
        print("üöÄ Starting Quick SVR Analysis Pipeline...")
        
        # Prepare data
        self.prepare_data(feature_columns, target_column)
        
        # Train or tune model
        if tune:
            print("\nüîß Hyperparameter tuning...")
            self.tune_hyperparameters(cv_folds=cv_folds, quick_mode=True)
        else:
            print("\nüîß Training default model...")
            self.train_model()
        
        # Auto feature selection if requested
        if auto_feature_selection:
            print("\nü§ñ Performing automatic feature selection...")
            feature_selection_results = self.auto_feature_selection(
                correlation_threshold=0.95,
                importance_threshold=0.01,
                retrain=True
            )
        
        # Generate report
        print("\nüìã Generating comprehensive report...")
        report = self.generate_report(include_cv=True, cv_folds=cv_folds)
        
        # Add feature selection results to report if performed
        if auto_feature_selection:
            report['feature_selection'] = feature_selection_results
        
        # Plot results
        print("\nüìä Creating visualizations...")
        self.plot_results()
        
        print("\n‚úÖ Analysis completed!")
        return report


# # Example usage with different workflows
# if __name__ == "__main__":
#     # Create SVR instance
#     svr = EnhancedSVR()
    
#     # Load your data
#     # svr.load_data('your_data.csv')
    
#     # Example with sample data
#     np.random.seed(42)
#     sample_data = {
#         'temperature': np.random.normal(300, 50, 200),
#         'pressure': np.random.normal(10, 2, 200),
#         'flow_rate': np.random.normal(100, 15, 200),
#         'concentration': np.random.normal(0.8, 0.2, 200),
#         'noise_feature': np.random.normal(0, 1, 200),  # Low importance
#         'yield': np.random.normal(75, 10, 200)
#     }
#     # Add some correlation
#     sample_data['temperature_copy'] = sample_data['temperature'] + np.random.normal(0, 5, 200)
    
#     svr.data = pd.DataFrame(sample_data)
#     feature_columns = ['temperature', 'pressure', 'flow_rate', 'concentration', 
#                       'noise_feature', 'temperature_copy']
#     target_column = 'yield'
    
#     print("üéØ ENHANCED SVR TOOL - WORKFLOW EXAMPLES")
#     print("="*50)
#     print("Choose your workflow:")
#     print("1. Quick Analysis (fast, automated)")
#     print("2. Interactive Workflow (step-by-step with choices)")
#     print("3. Auto Workflow (fully automated with intelligence)")
#     print("4. Guided Workflow (educational with explanations)")
#     print("\n" + "="*50)
    
#     # Example: Auto Workflow (recommended)
#     print("ü§ñ RUNNING AUTO WORKFLOW EXAMPLE...")
#     results = svr.auto_workflow(
#         feature_columns=feature_columns,
#         target_column=target_column,
#         optimization_target='balanced'  # 'performance', 'speed', or 'balanced'
#     )
    
    print("\nüéâ Enhanced SVR tool with multiple workflows is ready!")
    print("\nAvailable workflows:")
    print("‚Ä¢ quick_analysis() - Fast automated pipeline")  
    print("‚Ä¢ interactive_workflow() - Step-by-step with decisions")
    print("‚Ä¢ auto_workflow() - Fully automated with intelligence")
    print("‚Ä¢ guided_workflow() - Educational with explanations")
    print("\nFeature selection methods:")
    print("‚Ä¢ drop_features() - Manual feature dropping")
    print("‚Ä¢ drop_low_importance_features() - Remove unimportant features") 
    print("‚Ä¢ drop_highly_correlated_features() - Remove redundant features")
    print("‚Ä¢ auto_feature_selection() - Intelligent feature selection")
