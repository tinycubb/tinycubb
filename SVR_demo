from random import uniform
import pandas as pd
import numpy as np
from sklearn.svm import SVR
from scipy.stats import uniform

from sklearn.model_selection import RandomizedSearchCV, train_test_split, GridSearchCV, cross_val_score, cross_validate, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Tuple, Optional, Dict
import warnings
warnings.filterwarnings('ignore')

class SimpleAdjustableSVR:
    """
    A simple and adjustable SVR tool for regression tasks with comprehensive validation.
    """
    
    def __init__(self):
        self.data = None
        self.X = None
        self.y = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.model = None
        self.feature_names = None
        self.target_name = None
        self.best_params = None
        self.cv_results = None
        self.feature_importance = None
        
    def load_data(self, file_path: str) -> pd.DataFrame:
        """Load data from CSV file."""
        try:
            self.data = pd.read_csv(file_path)
            print(f"Data loaded successfully! Shape: {self.data.shape}")
            print(f"Columns: {list(self.data.columns)}")
            return self.data
        except Exception as e:
            print(f"Error loading data: {e}")
            return None
    
    def explore_data(self) -> None:
        """Basic data exploration."""
        if self.data is None:
            print("No data loaded. Please load data first.")
            return
        
        print("=" * 50)
        print("DATA EXPLORATION")
        print("=" * 50)
        
        print(f"Dataset shape: {self.data.shape}")
        print(f"\nData types:\n{self.data.dtypes}")
        print(f"\nMissing values:\n{self.data.isnull().sum()}")
        print(f"\nBasic statistics:\n{self.data.describe()}")
        
        # Correlation matrix
        plt.figure(figsize=(12, 10))
        correlation_matrix = self.data.corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')
        plt.title('Correlation Matrix')
        plt.tight_layout()
        plt.show()
    
    def prepare_features(self, feature_columns: List[str], target_column: str, 
                        test_size: float = 0.2, random_state: int = 42) -> None:
        """Prepare features and target for training."""
        if self.data is None:
            print("No data loaded. Please load data first.")
            return
        
        # Select features and target
        self.X = self.data[feature_columns].copy()
        self.y = self.data[target_column].copy()
        self.feature_names = feature_columns
        self.target_name = target_column
        
        # Handle missing values
        self.X = self.X.fillna(self.X.mean())
        self.y = self.y.fillna(self.y.mean())
        
        # Split data
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y, test_size=test_size, random_state=random_state
        )
        
        # Scale features
        self.X_train_scaled = self.scaler_X.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler_X.transform(self.X_test)
        
        # Scale target (optional but can help with some kernels)
        self.y_train_scaled = self.scaler_y.fit_transform(self.y_train.values.reshape(-1, 1)).ravel()
        self.y_test_scaled = self.scaler_y.transform(self.y_test.values.reshape(-1, 1)).ravel()
        
        print(f"Features prepared: {len(feature_columns)} features")
        print(f"Target: {target_column}")
        print(f"Training set: {self.X_train.shape[0]} samples")
        print(f"Test set: {self.X_test.shape[0]} samples")
    
    def train_simple_svr(self, kernel: str = 'rbf', C: float = 1.0, 
                        gamma: str = 'scale', epsilon: float = 0.1) -> None:
        """Train a simple SVR model with specified parameters."""
        if self.X_train is None:
            print("No training data prepared. Please prepare features first.")
            return
        
        # Create and train model
        self.model = SVR(kernel=kernel, C=C, gamma=gamma, epsilon=epsilon)
        self.model.fit(self.X_train_scaled, self.y_train_scaled)
        
        print(f"SVR model trained with:")
        print(f"  Kernel: {kernel}")
        print(f"  C: {C}")
        print(f"  Gamma: {gamma}")
        print(f"  Epsilon: {epsilon}")
        print(f"  Number of support vectors: {self.model.support_vectors_.shape[0]}")
        
        # Evaluate model
        self.evaluate_model()
    
    def tune_hyperparameters(self, param_grid: Optional[Dict] = None, 
                           cv: int = 3, scoring: str = 'neg_mean_squared_error',
                           quick_tune: bool = True, max_iter: int = 1000) -> None:
        """Tune hyperparameters using GridSearchCV with optimizations."""
        if self.X_train is None:
            print("No training data prepared. Please prepare features first.")
            return
        
        # Choose parameter grid based on quick_tune setting
        if param_grid is None:
            if quick_tune:
                # Smaller, faster parameter grid
                param_grid = {
                    'kernel': ['rbf', 'linear'],
                    'C': [0.1, 1, 10, 100],
                    'gamma': ['scale', 'auto'],
                    'epsilon': [0.01, 0.1, 0.2]
                }
                print("üöÄ Using QUICK TUNE mode (faster, fewer parameters)")
            else:
                # Comprehensive parameter grid
                param_grid = {
                    'kernel': ['rbf', 'linear', 'poly'],
                    'C': [0.01, 0.1, 1, 10],
                    # 'C' : uniform(0.01, 10),  # Random float for C
                    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],
                    'epsilon': [0.001, 0.01, 0.1, 0.2, 0.5]
                }
                print("üîç Using COMPREHENSIVE TUNE mode (slower, more thorough)")
        
        # Calculate total combinations
        total_combinations = 1
        for key, values in param_grid.items():
            total_combinations *= len(values)
        
        estimated_time = total_combinations * cv * 2  # rough estimate in seconds
        
        print(f"Parameter grid: {param_grid}")
        print(f"Total parameter combinations: {total_combinations}")
        print(f"Cross-validation folds: {cv}")
        print(f"Estimated time: ~{estimated_time//60:.0f}-{estimated_time//30:.0f} minutes")
        
        # Ask for confirmation if it will take too long
        if total_combinations > 200:
            print("‚ö†Ô∏è  WARNING: This will test many combinations and may take a long time!")
            print("üí° TIP: Set quick_tune=True for faster results")
            
        print("\nüîÑ Starting hyperparameter tuning...")
        
        # Create SVR with max_iter to prevent infinite loops
        base_svr = SVR(max_iter=max_iter)
        
        # Grid search with timeout and progress tracking
        grid_search = GridSearchCV(
            base_svr, 
            param_grid, 
            cv=cv, 
            scoring=scoring, 
            n_jobs=-1, 
            verbose=2,  # More detailed progress
            return_train_score=True,
            error_score='raise'  # Stop on errors instead of continuing
        )
        
        import time
        start_time = time.time()
        
        try:
            grid_search.fit(self.X_train_scaled, self.y_train_scaled)
            
            # Store best model and parameters
            self.model = grid_search.best_estimator_
            self.best_params = grid_search.best_params_
            
            end_time = time.time()
            actual_time = end_time - start_time
            
            print(f"\n‚úÖ Hyperparameter tuning completed!")
            print(f"‚è±Ô∏è  Actual time taken: {actual_time//60:.0f}m {actual_time%60:.0f}s")
            print(f"üéØ Best parameters: {self.best_params}")
            print(f"üìä Best cross-validation score: {grid_search.best_score_:.4f}")
            
            # Show top 5 results
            self._show_top_results(grid_search)
            
            # Evaluate model
            self.evaluate_model()
            
        except Exception as e:
            print(f"‚ùå Error during hyperparameter tuning: {e}")
            print("üí° Try reducing the parameter grid or using quick_tune=True")
            
    def _show_top_results(self, grid_search, top_n: int = 5):
        """Show top N parameter combinations from grid search."""
        results_df = pd.DataFrame(grid_search.cv_results_)
        
        # Sort by test score (best first)
        results_df = results_df.sort_values('mean_test_score', ascending=False)
        
        print(f"\nüèÜ Top {top_n} parameter combinations:")
        print("-" * 80)
        
        for i in range(min(top_n, len(results_df))):
            row = results_df.iloc[i]
            params = row['params']
            score = row['mean_test_score']
            std = row['std_test_score']
            
            print(f"{i+1}. Score: {score:.4f} (¬±{std:.4f})")
            print(f"   Parameters: {params}")
            print()
    
    def quick_tune(self, kernel_focus: str = 'rbf') -> None:
        """Quick hyperparameter tuning focused on one kernel."""
        print(f"üöÄ Quick tuning focused on {kernel_focus} kernel...")
        
        if kernel_focus == 'rbf':
            param_grid = {
                'kernel': ['rbf'],
                'C': [0.1, 1, 10, 100],
                'gamma': ['scale', 'auto', 0.01, 0.1],
                'epsilon': [0.01, 0.1, 0.2]
            }
        elif kernel_focus == 'linear':
            param_grid = {
                'kernel': ['linear'],
                'C': [0.1, 1, 10, 100],
                'epsilon': [0.01, 0.1, 0.2]
            }
        else:
            param_grid = {
                'kernel': [kernel_focus],
                'C': [0.1, 1, 10, 100],
                'epsilon': [0.01, 0.1, 0.2]
            }
        
        self.tune_hyperparameters(param_grid=param_grid, cv=3, quick_tune=True)
    
    def evaluate_model(self,  suppress_output=False) -> Dict:
        """Evaluate the trained model and check for overfitting."""
        if self.model is None:
            if not suppress_output:
                print("No model trained. Please train a model first.")
            return None
        
        # Make predictions
        y_train_pred_scaled = self.model.predict(self.X_train_scaled)
        y_test_pred_scaled = self.model.predict(self.X_test_scaled)
        
        # Transform back to original scale
        y_train_pred = self.scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).ravel()
        y_test_pred = self.scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).ravel()
        
        # Calculate metrics
        train_mse = mean_squared_error(self.y_train, y_train_pred)
        test_mse = mean_squared_error(self.y_test, y_test_pred)
        train_rmse = np.sqrt(train_mse)
        test_rmse = np.sqrt(test_mse)
        train_mae = mean_absolute_error(self.y_train, y_train_pred)
        test_mae = mean_absolute_error(self.y_test, y_test_pred)
        train_r2 = r2_score(self.y_train, y_train_pred)
        test_r2 = r2_score(self.y_test, y_test_pred)
        
        # Calculate percentage differences for overfitting analysis
        mse_diff_pct = ((test_mse - train_mse) / train_mse) * 100
        rmse_diff_pct = ((test_rmse - train_rmse) / train_rmse) * 100
        mae_diff_pct = ((test_mae - train_mae) / train_mae) * 100
        r2_diff_pct = ((train_r2 - test_r2) / abs(train_r2)) * 100 if train_r2 != 0 else 0
        
        metrics = {
            'train_mse': train_mse,
            'test_mse': test_mse,
            'train_rmse': train_rmse,
            'test_rmse': test_rmse,
            'train_mae': train_mae,
            'test_mae': test_mae,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'mse_diff_pct': mse_diff_pct,
            'rmse_diff_pct': rmse_diff_pct,
            'mae_diff_pct': mae_diff_pct,
            'r2_diff_pct': r2_diff_pct
        }
        if not suppress_output:
        # Print results with overfitting analysis
            print("\n" + "=" * 60)
            print("MODEL EVALUATION & OVERFITTING ANALYSIS")
            print("=" * 60)
            
            # Performance metrics
            print("PERFORMANCE METRICS:")
            print(f"  Training R¬≤:   {train_r2:.4f}")
            print(f"  Test R¬≤:       {test_r2:.4f}")
            print(f"  Training RMSE: {train_rmse:.4f}")
            print(f"  Test RMSE:     {test_rmse:.4f}")
            print(f"  Training MAE:  {train_mae:.4f}")
            print(f"  Test MAE:      {test_mae:.4f}")
            
            # Overfitting analysis
            print("\nOVERFITTING ANALYSIS:")
            print(f"  MSE difference:  {mse_diff_pct:+.2f}%")
            print(f"  RMSE difference: {rmse_diff_pct:+.2f}%")
            print(f"  MAE difference:  {mae_diff_pct:+.2f}%")
            print(f"  R¬≤ drop:         {r2_diff_pct:+.2f}%")
            
            # Overfitting interpretation
            print("\nOVERFITTING ASSESSMENT:")
            overfitting_score = self._assess_overfitting(mse_diff_pct, rmse_diff_pct, mae_diff_pct, r2_diff_pct)
            
            if overfitting_score == "Low":
                print("  ‚úÖ LOW OVERFITTING - Model generalizes well")
                print("     The model performs similarly on training and test data.")
            elif overfitting_score == "Moderate":
                print("  ‚ö†Ô∏è  MODERATE OVERFITTING - Some generalization issues")
                print("     The model performs noticeably better on training data.")
                print("     Consider: reducing C, increasing epsilon, or using simpler kernel.")
            else:
                print("  ‚ùå HIGH OVERFITTING - Poor generalization")
                print("     The model is memorizing training data rather than learning patterns.")
                print("     Recommendations: reduce C significantly, increase epsilon, use linear kernel.")
            
            # Additional recommendations
            print("\nRECOMMENDATIONS:")
            if test_r2 < 0.5:
                print("  ‚Ä¢ Low R¬≤ suggests model may be underfitting - try increasing C or using RBF kernel")
            if abs(mse_diff_pct) > 50:
                print("  ‚Ä¢ Large MSE difference suggests overfitting - reduce C or increase epsilon")
            if train_r2 > 0.95 and test_r2 < 0.8:
                print("  ‚Ä¢ Very high training R¬≤ with low test R¬≤ indicates severe overfitting")
            
            # Plot results
            self.plot_predictions(y_train_pred, y_test_pred)
        
        return metrics
    
    def _assess_overfitting(self, mse_diff_pct: float, rmse_diff_pct: float, 
                           mae_diff_pct: float, r2_diff_pct: float) -> str:
        """Assess the level of overfitting based on performance differences."""
        # Weight the different metrics
        avg_error_increase = (mse_diff_pct + rmse_diff_pct + mae_diff_pct) / 3
        
        # Criteria for overfitting levels
        if avg_error_increase < 10 and r2_diff_pct < 10:
            return "Low"
        elif avg_error_increase < 25 and r2_diff_pct < 20:
            return "Moderate"
        else:
            return "High"
    
    def plot_predictions(self, y_train_pred: np.ndarray, y_test_pred: np.ndarray) -> None:
        """Plot actual vs predicted values with enhanced overfitting visualization."""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # Training predictions
        axes[0, 0].scatter(self.y_train, y_train_pred, alpha=0.6, color='blue', s=50)
        axes[0, 0].plot([self.y_train.min(), self.y_train.max()], 
                       [self.y_train.min(), self.y_train.max()], 'r--', lw=2)
        train_r2 = r2_score(self.y_train, y_train_pred)
        axes[0, 0].set_xlabel('Actual')
        axes[0, 0].set_ylabel('Predicted')
        axes[0, 0].set_title(f'Training Set: Actual vs Predicted (R¬≤ = {train_r2:.4f})')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Test predictions
        axes[0, 1].scatter(self.y_test, y_test_pred, alpha=0.6, color='green', s=50)
        axes[0, 1].plot([self.y_test.min(), self.y_test.max()], 
                       [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
        test_r2 = r2_score(self.y_test, y_test_pred)
        axes[0, 1].set_xlabel('Actual')
        axes[0, 1].set_ylabel('Predicted')
        axes[0, 1].set_title(f'Test Set: Actual vs Predicted (R¬≤ = {test_r2:.4f})')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Training residuals
        train_residuals = self.y_train - y_train_pred
        axes[1, 0].scatter(y_train_pred, train_residuals, alpha=0.6, color='blue', s=50)
        axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)
        axes[1, 0].set_xlabel('Predicted')
        axes[1, 0].set_ylabel('Residuals')
        train_rmse = np.sqrt(mean_squared_error(self.y_train, y_train_pred))
        axes[1, 0].set_title(f'Training Residuals (RMSE = {train_rmse:.4f})')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Test residuals
        test_residuals = self.y_test - y_test_pred
        axes[1, 1].scatter(y_test_pred, test_residuals, alpha=0.6, color='green', s=50)
        axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)
        axes[1, 1].set_xlabel('Predicted')
        axes[1, 1].set_ylabel('Residuals')
        test_rmse = np.sqrt(mean_squared_error(self.y_test, y_test_pred))
        axes[1, 1].set_title(f'Test Residuals (RMSE = {test_rmse:.4f})')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # Additional overfitting visualization
        self._plot_overfitting_comparison(y_train_pred, y_test_pred)
    
    def _plot_overfitting_comparison(self, y_train_pred: np.ndarray, y_test_pred: np.ndarray) -> None:
        """Create additional plots to visualize overfitting."""
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        # Performance comparison bar chart
        train_r2 = r2_score(self.y_train, y_train_pred)
        test_r2 = r2_score(self.y_test, y_test_pred)
        train_rmse = np.sqrt(mean_squared_error(self.y_train, y_train_pred))
        test_rmse = np.sqrt(mean_squared_error(self.y_test, y_test_pred))
        train_mae = mean_absolute_error(self.y_train, y_train_pred)
        test_mae = mean_absolute_error(self.y_test, y_test_pred)
        
        metrics = ['R¬≤', 'RMSE', 'MAE']
        train_values = [train_r2, train_rmse, train_mae]
        test_values = [test_r2, test_rmse, test_mae]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        axes[0].bar(x - width/2, train_values, width, label='Training', color='skyblue', alpha=0.8)
        axes[0].bar(x + width/2, test_values, width, label='Test', color='lightcoral', alpha=0.8)
        axes[0].set_xlabel('Metrics')
        axes[0].set_ylabel('Values')
        axes[0].set_title('Training vs Test Performance')
        axes[0].set_xticks(x)
        axes[0].set_xticklabels(metrics)
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Error distribution comparison
        train_errors = abs(self.y_train - y_train_pred)
        test_errors = abs(self.y_test - y_test_pred)
        
        axes[1].hist(train_errors, bins=15, alpha=0.7, label='Training Errors', color='skyblue', density=True)
        axes[1].hist(test_errors, bins=15, alpha=0.7, label='Test Errors', color='lightcoral', density=True)
        axes[1].set_xlabel('Absolute Error')
        axes[1].set_ylabel('Density')
        axes[1].set_title('Error Distribution Comparison')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # Performance gap visualization
        r2_gap = train_r2 - test_r2
        rmse_gap = test_rmse - train_rmse
        mae_gap = test_mae - train_mae
        
        gaps = [r2_gap, rmse_gap, mae_gap]
        gap_labels = ['R¬≤ Gap\n(Train - Test)', 'RMSE Gap\n(Test - Train)', 'MAE Gap\n(Test - Train)']
        colors = ['red' if gap > 0 else 'green' for gap in gaps]
        
        bars = axes[2].bar(gap_labels, gaps, color=colors, alpha=0.7)
        axes[2].set_ylabel('Performance Gap')
        axes[2].set_title('Overfitting Indicators')
        axes[2].axhline(y=0, color='black', linestyle='-', linewidth=1)
        axes[2].grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar, gap in zip(bars, gaps):
            height = bar.get_height()
            axes[2].text(bar.get_x() + bar.get_width()/2., height + (0.01 if height > 0 else -0.01),
                        f'{gap:.4f}', ha='center', va='bottom' if height > 0 else 'top')
        
        plt.tight_layout()
        plt.show()

    def k_fold_cross_validation(self, k: int = 5, model_params: Optional[Dict] = None,
                               scoring: List[str] = ['neg_mean_squared_error', 'r2', 'neg_mean_absolute_error'],
                               stratified: bool = False, random_state: int = 42) -> Dict:
        """Perform comprehensive K-fold cross-validation."""
        if self.X is None:
            print("No data prepared. Please prepare features first.")
            return None
        
        # Use all data for cross-validation (not just train set)
        X_scaled = self.scaler_X.fit_transform(self.X)
        y_scaled = self.scaler_y.fit_transform(self.y.values.reshape(-1, 1)).ravel()
        
        # Set default model parameters
        if model_params is None:
            model_params = {'kernel': 'rbf', 'C': 100, 'gamma': 'scale', 'epsilon': 0.1}
        
        # Create SVR model
        model = SVR(**model_params)
        
        # Choose cross-validation strategy
        if stratified:
            # For regression, we'll use binned stratification
            from sklearn.model_selection import StratifiedKFold
            # Create bins for stratification
            y_binned = pd.cut(self.y, bins=k, labels=False)
            cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=random_state)
            cv_splits = cv.split(X_scaled, y_binned)
        else:
            cv = KFold(n_splits=k, shuffle=True, random_state=random_state)
            cv_splits = cv.split(X_scaled, y_scaled)
        
        print(f"üîÑ Performing {k}-Fold Cross-Validation...")
        print(f"üìä Model parameters: {model_params}")
        print(f"üìà Scoring metrics: {scoring}")
        
        # Perform cross-validation
        cv_results = cross_validate(
            model, X_scaled, y_scaled, 
            cv=cv, scoring=scoring, 
            return_train_score=True, 
            return_estimator=True,
            n_jobs=-1
        )
        
        # Store results
        self.cv_results = cv_results
        
        # Calculate statistics
        results_summary = {}
        for metric in scoring:
            test_scores = cv_results[f'test_{metric}']
            train_scores = cv_results[f'train_{metric}']
            
            # Convert negative scores back to positive
            if 'neg_' in metric:
                test_scores = -test_scores
                train_scores = -train_scores
                clean_metric = metric.replace('neg_', '')
            else:
                clean_metric = metric
            
            results_summary[f'{clean_metric}_test_mean'] = np.mean(test_scores)
            results_summary[f'{clean_metric}_test_std'] = np.std(test_scores)
            results_summary[f'{clean_metric}_train_mean'] = np.mean(train_scores)
            results_summary[f'{clean_metric}_train_std'] = np.std(train_scores)
            results_summary[f'{clean_metric}_test_scores'] = test_scores
            results_summary[f'{clean_metric}_train_scores'] = train_scores
        
        # Print results
        self._print_cv_results(results_summary, k)
        
        # Plot results
        self._plot_cv_results(results_summary, k)
        
        return results_summary
    
    def _print_cv_results(self, results: Dict, k: int) -> None:
        """Print formatted cross-validation results."""
        print("\n" + "=" * 70)
        print(f"{k}-FOLD CROSS-VALIDATION RESULTS")
        print("=" * 70)
        
        metrics = ['mean_squared_error', 'r2', 'mean_absolute_error']
        
        for metric in metrics:
            if f'{metric}_test_mean' in results:
                test_mean = results[f'{metric}_test_mean']
                test_std = results[f'{metric}_test_std']
                train_mean = results[f'{metric}_train_mean']
                train_std = results[f'{metric}_train_std']
                
                print(f"\nüìä {metric.upper().replace('_', ' ')}:")
                print(f"   Test:  {test_mean:.4f} ¬± {test_std:.4f}")
                print(f"   Train: {train_mean:.4f} ¬± {train_std:.4f}")
                
                # Calculate overfitting indicator
                if metric != 'r2':
                    overfitting = ((test_mean - train_mean) / train_mean) * 100
                else:
                    overfitting = ((train_mean - test_mean) / abs(train_mean)) * 100 if train_mean != 0 else 0
                
                print(f"   Gap:   {overfitting:+.2f}%")
        
        # Overall assessment
        r2_gap = ((results['r2_train_mean'] - results['r2_test_mean']) / abs(results['r2_train_mean'])) * 100
        mse_gap = ((results['mean_squared_error_test_mean'] - results['mean_squared_error_train_mean']) / results['mean_squared_error_train_mean']) * 100
        
        print(f"\nüéØ CROSS-VALIDATION ASSESSMENT:")
        if r2_gap < 10 and mse_gap < 15:
            print("   ‚úÖ EXCELLENT - Model shows consistent performance across folds")
        elif r2_gap < 20 and mse_gap < 30:
            print("   ‚ö†Ô∏è  GOOD - Minor overfitting detected")
        else:
            print("   ‚ùå CONCERNING - Significant overfitting across folds")
        
        print(f"   üìà Average Test R¬≤: {results['r2_test_mean']:.4f}")
        print(f"   üìâ R¬≤ Consistency: {results['r2_test_std']:.4f} (lower is better)")
    
    def _plot_cv_results(self, results: Dict, k: int) -> None:
        """Plot cross-validation results."""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. R¬≤ scores across folds
        folds = range(1, k + 1)
        r2_test = results['r2_test_scores']
        r2_train = results['r2_train_scores']
        
        axes[0, 0].plot(folds, r2_test, 'o-', label='Test', color='green', linewidth=2, markersize=8)
        axes[0, 0].plot(folds, r2_train, 's-', label='Train', color='blue', linewidth=2, markersize=8)
        axes[0, 0].axhline(y=np.mean(r2_test), color='green', linestyle='--', alpha=0.7, label=f'Test Mean: {np.mean(r2_test):.3f}')
        axes[0, 0].axhline(y=np.mean(r2_train), color='blue', linestyle='--', alpha=0.7, label=f'Train Mean: {np.mean(r2_train):.3f}')
        axes[0, 0].set_xlabel('Fold')
        axes[0, 0].set_ylabel('R¬≤ Score')
        axes[0, 0].set_title('R¬≤ Scores Across Folds')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. RMSE scores across folds
        mse_test = results['mean_squared_error_test_scores']
        mse_train = results['mean_squared_error_train_scores']
        rmse_test = np.sqrt(mse_test)
        rmse_train = np.sqrt(mse_train)
        
        axes[0, 1].plot(folds, rmse_test, 'o-', label='Test', color='red', linewidth=2, markersize=8)
        axes[0, 1].plot(folds, rmse_train, 's-', label='Train', color='orange', linewidth=2, markersize=8)
        axes[0, 1].axhline(y=np.mean(rmse_test), color='red', linestyle='--', alpha=0.7, label=f'Test Mean: {np.mean(rmse_test):.3f}')
        axes[0, 1].axhline(y=np.mean(rmse_train), color='orange', linestyle='--', alpha=0.7, label=f'Train Mean: {np.mean(rmse_train):.3f}')
        axes[0, 1].set_xlabel('Fold')
        axes[0, 1].set_ylabel('RMSE')
        axes[0, 1].set_title('RMSE Across Folds')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. Box plot of performance metrics
        metrics_data = [r2_test, rmse_test]
        metrics_labels = ['R¬≤ (Test)', 'RMSE (Test)']
        
        # Normalize RMSE for comparison (scale to 0-1 range like R¬≤)
        rmse_normalized = 1 - (rmse_test - np.min(rmse_test)) / (np.max(rmse_test) - np.min(rmse_test))
        
        axes[1, 0].boxplot([r2_test, rmse_normalized], labels=['R¬≤ (Test)', 'RMSE (Normalized)'])
        axes[1, 0].set_ylabel('Score')
        axes[1, 0].set_title('Performance Distribution Across Folds')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. Train vs Test comparison
        metrics = ['R¬≤', 'RMSE', 'MAE']
        train_means = [np.mean(r2_train), np.mean(rmse_train), np.mean(results['mean_absolute_error_train_scores'])]
        test_means = [np.mean(r2_test), np.mean(rmse_test), np.mean(results['mean_absolute_error_test_scores'])]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        bars1 = axes[1, 1].bar(x - width/2, train_means, width, label='Train', color='skyblue', alpha=0.8)
        bars2 = axes[1, 1].bar(x + width/2, test_means, width, label='Test', color='lightcoral', alpha=0.8)
        
        axes[1, 1].set_xlabel('Metrics')
        axes[1, 1].set_ylabel('Values')
        axes[1, 1].set_title('Train vs Test Performance (CV Average)')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(metrics)
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,
                               f'{height:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
    
    def learning_curve_analysis(self, train_sizes: np.ndarray = None, 
                               model_params: Optional[Dict] = None, cv: int = 5) -> Dict:
        """Analyze learning curves to detect overfitting/underfitting."""
        from sklearn.model_selection import learning_curve
        
        if self.X is None:
            print("No data prepared. Please prepare features first.")
            return None
        
        if train_sizes is None:
            train_sizes = np.linspace(0.1, 1.0, 10)
        
        if model_params is None:
            model_params = {'kernel': 'rbf', 'C': 100, 'gamma': 'scale', 'epsilon': 0.1}
        
        print("üìà Analyzing learning curves...")
        
        # Prepare data
        X_scaled = self.scaler_X.fit_transform(self.X)
        y_scaled = self.scaler_y.fit_transform(self.y.values.reshape(-1, 1)).ravel()
        
        # Create model
        model = SVR(**model_params)
        
        # Generate learning curve
        train_sizes_abs, train_scores, test_scores = learning_curve(
            model, X_scaled, y_scaled, 
            train_sizes=train_sizes,
            cv=cv, scoring='r2', n_jobs=-1, random_state=42
        )
        
        # Calculate means and stds
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        test_mean = np.mean(test_scores, axis=1)
        test_std = np.std(test_scores, axis=1)
        
        # Plot learning curves
        plt.figure(figsize=(12, 8))
        
        plt.plot(train_sizes_abs, train_mean, 'o-', color='blue', label='Training Score')
        plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')
        
        plt.plot(train_sizes_abs, test_mean, 'o-', color='red', label='Validation Score')
        plt.fill_between(train_sizes_abs, test_mean - test_std, test_mean + test_std, alpha=0.2, color='red')
        
        plt.xlabel('Training Set Size')
        plt.ylabel('R¬≤ Score')
        plt.title('Learning Curves')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Add interpretation
        final_gap = train_mean[-1] - test_mean[-1]
        plt.text(0.02, 0.98, f'Final Train-Test Gap: {final_gap:.3f}', 
                transform=plt.gca().transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        if final_gap > 0.1:
            plt.text(0.02, 0.90, '‚ö†Ô∏è High overfitting detected', 
                    transform=plt.gca().transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))
        elif final_gap > 0.05:
            plt.text(0.02, 0.90, '‚ö†Ô∏è Moderate overfitting detected', 
                    transform=plt.gca().transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
        else:
            plt.text(0.02, 0.90, '‚úÖ Good generalization', 
                    transform=plt.gca().transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
        
        plt.tight_layout()
        plt.show()
        
        return {
            'train_sizes': train_sizes_abs,
            'train_scores_mean': train_mean,
            'train_scores_std': train_std,
            'test_scores_mean': test_mean,
            'test_scores_std': test_std,
            'final_gap': final_gap
        }

    def feature_importance_analysis(self, method: str = 'permutation', n_repeats: int = 10) -> Dict:
        """Analyze feature importance using different methods."""
        if self.model is None:
            print("No model trained. Please train a model first.")
            return None
        
        from sklearn.inspection import permutation_importance
        
        print(f"üîç Analyzing feature importance using {method} method...")
        
        if method == 'permutation':
            # Permutation importance
            perm_importance = permutation_importance(
                self.model, self.X_test_scaled, self.y_test_scaled,
                n_repeats=n_repeats, random_state=42, n_jobs=-1
            )
            
            importance_mean = perm_importance.importances_mean
            importance_std = perm_importance.importances_std
            
        elif method == 'coefficient' and hasattr(self.model, 'coef_'):
            # For linear SVR, use coefficients
            if self.model.kernel == 'linear':
                importance_mean = np.abs(self.model.coef_[0])
                importance_std = np.zeros_like(importance_mean)
            else:
                print("Coefficient method only available for linear kernel. Using permutation instead.")
                return self.feature_importance_analysis(method='permutation', n_repeats=n_repeats)
        
        # Store results
        self.feature_importance = {
            'features': self.feature_names,
            'importance_mean': importance_mean,
            'importance_std': importance_std
        }
        
        # Create feature importance DataFrame
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance_mean,
            'std': importance_std
        }).sort_values('importance', ascending=False)
        
        # Print results
        print("\n" + "=" * 60)
        print("FEATURE IMPORTANCE ANALYSIS")
        print("=" * 60)
        print(importance_df.to_string(index=False, float_format='%.4f'))
        
        # Plot feature importance
        plt.figure(figsize=(12, 8))
        
        # Horizontal bar plot
        y_pos = np.arange(len(self.feature_names))
        sorted_idx = np.argsort(importance_mean)
        
        plt.barh(y_pos, importance_mean[sorted_idx], xerr=importance_std[sorted_idx], 
                alpha=0.7, capsize=5, color='skyblue', edgecolor='navy')
        plt.yticks(y_pos, [self.feature_names[i] for i in sorted_idx])
        plt.xlabel('Feature Importance')
        plt.title(f'Feature Importance ({method.title()} Method)')
        plt.grid(True, alpha=0.3, axis='x')
        
        # Add value labels
        for i, (importance, std) in enumerate(zip(importance_mean[sorted_idx], importance_std[sorted_idx])):
            plt.text(importance + std + 0.001, i, f'{importance:.3f}', 
                    va='center', ha='left', fontsize=9)
        
        plt.tight_layout()
        plt.show()
        
        return importance_df
    
    def model_validation_report(self, include_cv: bool = True, cv_folds: int = 5,
                               include_learning_curves: bool = True, 
                               include_feature_importance: bool = True) -> Dict:
        """Generate comprehensive model validation report."""
        if self.model is None:
            print("No model trained. Please train a model first.")
            return None
        
        print("üìã Generating Comprehensive Model Validation Report...")
        print("=" * 70)
        
        report = {}
        
        # 1. Basic model evaluation
        print("1Ô∏è‚É£ Basic Model Evaluation...")
        basic_metrics = self.evaluate_model()
        report['basic_metrics'] = basic_metrics
        
        # 2. Cross-validation analysis
        if include_cv:
            print("\n2Ô∏è‚É£ Cross-Validation Analysis...")
            model_params = self.model.get_params()
            cv_results = self.k_fold_cross_validation(k=cv_folds, model_params=model_params)
            report['cv_results'] = cv_results
        
        # 3. Learning curves
        if include_learning_curves:
            print("\n3Ô∏è‚É£ Learning Curve Analysis...")
            model_params = self.model.get_params()
            learning_results = self.learning_curve_analysis(model_params=model_params, cv=cv_folds)
            report['learning_curves'] = learning_results
        
        # 4. Feature importance
        if include_feature_importance:
            print("\n4Ô∏è‚É£ Feature Importance Analysis...")
            importance_results = self.feature_importance_analysis()
            report['feature_importance'] = importance_results
        
        # 5. Final recommendations
        print("\n5Ô∏è‚É£ Final Recommendations...")
        self._generate_recommendations(report)
        
        return report
    
    def _generate_recommendations(self, report: Dict) -> None:
        """Generate actionable recommendations based on validation results."""
        print("\n" + "=" * 70)
        print("üéØ ACTIONABLE RECOMMENDATIONS")
        print("=" * 70)
        
        recommendations = []
        
        # Check basic performance
        if 'basic_metrics' in report:
            test_r2 = report['basic_metrics']['test_r2']
            train_r2 = report['basic_metrics']['train_r2']
            r2_gap = train_r2 - test_r2
            
            if test_r2 < 0.5:
                recommendations.append("üìà LOW PERFORMANCE: Consider more complex model or feature engineering")
            elif test_r2 > 0.9:
                recommendations.append("üéØ EXCELLENT PERFORMANCE: Model is working very well")
            
            if r2_gap > 0.15:
                recommendations.append("‚ö†Ô∏è  HIGH OVERFITTING: Reduce C parameter or increase epsilon")
            elif r2_gap < 0.05:
                recommendations.append("‚úÖ GOOD GENERALIZATION: Model generalizes well to new data")
        
        # Check cross-validation consistency
        if 'cv_results' in report:
            r2_std = report['cv_results']['r2_test_std']
            if r2_std > 0.1:
                recommendations.append("üìä INCONSISTENT CV: Model performance varies significantly across folds")
            else:
                recommendations.append("‚úÖ CONSISTENT CV: Model shows stable performance across folds")
        
        # Check learning curves
        if 'learning_curves' in report:
            final_gap = report['learning_curves']['final_gap']
            if final_gap > 0.1:
                recommendations.append("üìö LEARNING CURVES: High train-test gap suggests overfitting")
            elif final_gap < 0.05:
                recommendations.append("‚úÖ LEARNING CURVES: Good learning progression detected")
        
        # Feature importance insights
        if 'feature_importance' in report:
            top_features = report['feature_importance'].head(3)['feature'].tolist()
            recommendations.append(f"üîç TOP FEATURES: Focus on {', '.join(top_features)}")
        
        # Print recommendations
        for i, rec in enumerate(recommendations, 1):
            print(f"{i:2d}. {rec}")
        
        # Overall model quality
        print(f"\nüèÜ OVERALL MODEL QUALITY:")
        if 'basic_metrics' in report:
            test_r2 = report['basic_metrics']['test_r2']
            if test_r2 > 0.8:
                print("    üåü EXCELLENT - Ready for production use")
            elif test_r2 > 0.6:
                print("    ‚úÖ GOOD - Suitable for most applications")
            elif test_r2 > 0.4:
                print("    ‚ö†Ô∏è  FAIR - Consider improvements before deployment")
            else:
                print("    ‚ùå POOR - Significant improvements needed")
    
    def auto_tune_and_validate(self, quick_mode: bool = True) -> Dict:
        """Automatically tune hyperparameters and perform full validation."""
        print("üöÄ Starting Automatic Tuning and Validation Pipeline...")
        
        # Step 1: Hyperparameter tuning
        if quick_mode:
            self.tune_hyperparameters(quick_tune=True, cv=3)
        else:
            self.tune_hyperparameters(quick_tune=False, cv=5)
        
        # Step 2: Full validation report
        report = self.model_validation_report(
            include_cv=True, 
            cv_folds=5,
            include_learning_curves=True,
            include_feature_importance=True
        )
        
        print("\n‚úÖ Automatic tuning and validation completed!")
        return report
    

    def auto_reduce_overfitting(self, 
                                C_values=[0.01, 0.1, 0.5, 1, 5], 
                                gamma_values=[0.001, 0.01, 0.05, 0.1, 'scale'], 
                                epsilon_values=[0.01, 0.05, 0.1, 0.2], 
                                verbose=False):
        """
        Grid search to find SVR parameters that minimize train-test R¬≤ gap.
        """
        best_gap = float('inf')
        best_params = None
        best_metrics = None
    
        for C in C_values:
            for gamma in gamma_values:
                for epsilon in epsilon_values:
                    self.train_simple_svr(kernel='rbf', C=C, gamma=gamma, epsilon=epsilon)
                    metrics = self.evaluate_model(suppress_output=True)
                    gap = abs(metrics['train_r2'] - metrics['test_r2'])
                    if verbose:
                        print(f"Tested: C={C}, gamma={gamma}, epsilon={epsilon} | Train R¬≤={metrics['train_r2']:.3f}, Test R¬≤={metrics['test_r2']:.3f}, Gap={gap:.3f}")
                    if gap < best_gap:
                        best_gap = gap
                        best_params = {'C': C, 'gamma': gamma, 'epsilon': epsilon}
                        best_metrics = metrics
    
        print("\nBest parameters for minimal train-test R¬≤ gap:")
        print(best_params)
        print(f"Train R¬≤: {best_metrics['train_r2']:.3f}, Test R¬≤: {best_metrics['test_r2']:.3f}, Gap: {best_gap:.3f}")
        return best_params, best_metrics
    
    def predict_new_data(self, new_data: pd.DataFrame) -> np.ndarray:
        """Make predictions on new data."""
        if self.model is None:
            print("No model trained. Please train a model first.")
            return None
        
        # Ensure new data has the same features
        new_data_features = new_data[self.feature_names]
        
        # Handle missing values
        new_data_features = new_data_features.fillna(new_data_features.mean())
        
        # Scale features
        new_data_scaled = self.scaler_X.transform(new_data_features)
        
        # Make predictions and transform back
        predictions_scaled = self.model.predict(new_data_scaled)
        predictions = self.scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).ravel()
        
        return predictions
    
    def save_model(self, file_path: str) -> None:
        """Save the trained model and scalers."""
        import joblib
        
        if self.model is None:
            print("No model trained. Please train a model first.")
            return
        
        model_data = {
            'model': self.model,
            'scaler_X': self.scaler_X,
            'scaler_y': self.scaler_y,
            'feature_names': self.feature_names,
            'target_name': self.target_name,
            'best_params': self.best_params
        }
        
        joblib.dump(model_data, file_path)
        print(f"Model saved to {file_path}")
    
    def load_model(self, file_path: str) -> None:
        """Load a saved model."""
        import joblib
        
        model_data = joblib.load(file_path)
        self.model = model_data['model']
        self.scaler_X = model_data['scaler_X']
        self.scaler_y = model_data['scaler_y']
        self.feature_names = model_data['feature_names']
        self.target_name = model_data['target_name']
        self.best_params = model_data.get('best_params', None)
        
        print(f"Model loaded from {file_path}")
        print(f"Features: {self.feature_names}")
        print(f"Target: {self.target_name}")

# # Example usage with enhanced features
# if __name__ == "__main__":
#     # Create SVR instance
#     svr_tool = SimpleAdjustableSVR()
    
#     # Example with sample data
#     sample_data = {
#         'Date': [43276, 43277, 43278, 43279, 43280, 43281],
#         'Rxtor Temp': [500.92, 496.04, 503.93, 504.59, 503.50, 501.57],
#         'Feed Temp': [298.79, 300.43, 303.32, 305.29, 304.25, 304.53],
#         'Conversion %': [76.21, 74.13, 78.20, 77.64, 77.15, 78.31],
#         'Cat Cir rate': [9.03, 8.69, 9.27, 8.66, 8.30, 8.26],
#         'C/O Ratio': [7.72, 7.17, 7.93, 7.49, 7.19, 7.26],
#         'OVHD Temp': [120.79, 119.98, 119.96, 119.57, 120.15, 121.48],
#         'TCR flow': [3854.95, 3754.83, 3473.65, 3644.27, 3306.96, 3304.67],
#         'Atomizer Flow': [37.23, 32.45, 32.37, 32.39, 32.43, 32.35],
#         'stab reflux': [393.15, 386.16, 393.32, 383.57, 373.35, 395.64],
#         'CCG Yield': [53.68, 52.83, 55.20, 54.75, 54.48, 55.84],
#         'feed flow': [1513.50, 1544.40, 1607.34, 1612.48, 1608.24, 1605.60]
#     }
    
#     svr_tool.data = pd.DataFrame(sample_data)
    
#     # Prepare features
#     feature_columns = ['Rxtor Temp', 'Feed Temp', 'Conversion %', 'Cat Cir rate', 
#                       'C/O Ratio', 'OVHD Temp', 'TCR flow', 'Atomizer Flow', 
#                       'stab reflux', 'feed flow']
#     target_column = 'CCG Yield'
    
#     svr_tool.prepare_features(feature_columns, target_column)
    
#     # Train model
#     svr_tool.train_simple_svr(kernel='rbf', C=100, gamma='scale', epsilon=0.1)
    
#     print("üéâ Enhanced SVR tool ready for advanced analysis!")
